{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDgK4tUhfagHd4BfweCXSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/painting-lora-finetune/blob/main/neural_style_transfer_photos_to_paintings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Neural style transfer — brief**\n",
        "**Goal**<br>\n",
        "Turn your photo into a painting-like image **without moving objects**: keep the photo’s layout, borrow the painting’s colors/texture.\n",
        "\n",
        "**How it works (high level)**<br>\n",
        "We create an output image `X` and **optimize its pixels** so that:<br>\n",
        "• its **deep features** (from a CNN) match the photo → preserves structure/content;<br>\n",
        "• its **feature statistics** (Gram matrices) match the painting → transfers style/brushwork.\n",
        "\n",
        "**VGG-19**<br>\n",
        "**VGG-19** is a classic **convolutional neural network (CNN)** trained on ImageNet. It stacks many conv layers that detect edges, textures, parts, and objects.<br>\n",
        "• **Shallow layers** respond to color/texture; **deep layers** to object/layout.<br>\n",
        "• We **freeze** VGG (no training) and use it only to extract features that act as our “perceptual rulers.”<br>\n",
        "• Matching **deep** VGG features → keeps object placements. Matching **Gram stats** across layers → injects painting style.\n",
        "\n",
        "<h3><font color=\"#0b3d91\">How the neural style transfer model is trained (fast/feed-forward version)</font></h3>\n",
        "\n",
        "**Notation**\n",
        "- `f_phi`: trainable **stylizer** CNN (the only network you update)\n",
        "- `VGG`: frozen **loss network** (used only to compute losses)\n",
        "- `C`: content image, `S`: style image\n",
        "- `X = f_phi(C)`: stylized output\n",
        "\n",
        "**Training loop**\n",
        "1. **Freeze VGG.** Use it only to extract features for the losses (no weight updates).\n",
        "2. **Choose a style** image `S` (or sample from a style set).\n",
        "3. **For each content image `C`:**\n",
        "    - **Forward pass:** compute `X = f_phi(C)` (run `C` through the stylizer to get the current stylized guess).\n",
        "    - **Extract features (with frozen VGG):** get feature maps `F_l(C)`, `F_l(X)`, and `F_l(S)` from selected layers `l`.\n",
        "    - **Content loss:** make `F_l(X)` close to `F_l(C)` on deeper layers (preserves layout/structure).\n",
        "    - **Style loss:** make **Gram matrices** of `X` match those of `S` across chosen layers (captures texture/brushwork).\n",
        "    - **TV loss (optional):** add a small total-variation penalty on `X` to encourage smoothness.\n",
        "    - **Total loss:** `L = lambda_c * L_content + lambda_s * L_style + lambda_tv * L_tv`.\n",
        "    - **Backprop:** send gradients through VGG into `f_phi` (VGG stays frozen) and **update only `phi`** (e.g., Adam).\n",
        "4. **Repeat** over batches until validation loss plateaus. The trained `f_phi` then stylizes any new `C` in a single forward pass.\n",
        "\n",
        "> **Alternative (optimization-based NST):** don’t learn `f_phi`. Initialize `X` (e.g., `X <- C`) and **optimize the pixels of `X`** directly to minimize the same losses with VGG frozen.\n",
        "\n",
        "\n",
        "**Pipeline**<br>\n",
        "Load & normalize images → run through frozen VGG → compute **content loss** (deep layer), **style loss** (Gram matrices across several layers), plus tiny **total variation (TV)** smoothing → backprop gradients **to the image pixels** with L-BFGS/Adam until it looks right.\n",
        "\n",
        "**Total Variation (TV) smoothing** - A regularizer that penalizes rapid pixel-to-pixel changes. It prefers images that are locally smooth (piecewise-smooth) without simply blurring everything."
      ],
      "metadata": {
        "id": "-0NeC4NdFWWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font color=\"#0b3d91\">Gram matrices (style loss)</font></h3>\n",
        "\n",
        "**Setup (for one VGG layer `l`)**\n",
        "- Feature maps: `F_l(X)` has shape `[C, H, W]` (C channels, H×W spatial).\n",
        "- Flatten spatial dims: `F = reshape(F_l(X), [C, H*W])`.\n",
        "- **Gram matrix (per layer):**\n",
        "  - Start with features `F_l(X)` of shape `[C, H, W]`.\n",
        "  - Flatten spatial dims → `F` has shape `[C, N]`, where `N = H * W`.\n",
        "  - Indices: `i, j ∈ {1..C}` are **channel indices**; `p ∈ {1..N}` is the **pixel (spatial) index** after flattening.\n",
        "  - Define the `[C, C]` matrix `G` by  \n",
        "    `G[i, j] = (1/N) * sum_{p=1..N} F[i, p] * F[j, p]`.\n",
        "  - This is the **average product across all spatial positions** of channels `i` and `j`.  \n",
        "    Averaging over `p` removes location and keeps **co-activation (texture) statistics**.\n",
        "\n",
        "**Intuition**\n",
        "- Entry `G[i, j]` is the dot product between channel `i` and `j` across all pixels → how strongly they **co-activate**.\n",
        "- Averaging over locations discards exact positions, so Gram matrices capture **texture/brushwork statistics** (which features occur together), not layout.\n",
        "- Matching `G_l(X)` to `G_l(S)` makes `X` use the same color/texture patterns as the style image at that layer’s scale.\n",
        "\n",
        "**Why multiple layers?**\n",
        "- Shallow layers → fine textures/colors; deeper layers → broader patterns.\n",
        "- Summing style losses over several layers gives a **multi-scale** style match.\n",
        "\n",
        "**Style loss formula**\n",
        "- Per layer: `L_style_l = || G_l(X) - G_l(S) ||_F^2`\n",
        "- Total style loss: weighted sum over chosen layers.\n",
        "- Normalize by `H*W` (or `C*H*W`) to keep values scale-stable."
      ],
      "metadata": {
        "id": "LnNbZSB3Br4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "JkuuncyXyG4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6LSFUnVPsgd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "306aee68-ed0a-4213-a153-d5768d58a7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 | Vision: 0.23.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Colab usually has recent torch/torchvision, but this is safe.\n",
        "!pip -q install --upgrade torch torchvision pillow\n",
        "import torch, torchvision, PIL\n",
        "print(\"Torch:\", torch.__version__, \"| Vision:\", torchvision.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "WSqes22jy00u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Set device and ImageNet normalization (for VGG-19 features)**\n",
        "\n",
        "- **Device selection:** Use GPU (`cuda`) if available; otherwise fall back to CPU. All tensors/ops must be on the **same device**.\n",
        "- **ImageNet normalization:** VGG-19 expects RGB inputs scaled to **[0,1]** and normalized per channel with:\n",
        "  - mean = `[0.485, 0.456, 0.406]`\n",
        "  - std  = `[0.229, 0.224, 0.225]`\n",
        "<br>We apply it as: `(x - mean) / std` (broadcast per channel over H×W).\n",
        "- **Why:** Feeding the exact normalization used in training keeps VGG feature distributions correct; skipping it can cause unstable optimization or odd colors.\n",
        "- **`.to(device)` on mean/std:** Puts these constants on the same device as your images to avoid device-mismatch errors."
      ],
      "metadata": {
        "id": "8OG7pKx2zG8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).to(device)"
      ],
      "metadata": {
        "id": "8Alx0zfjy47k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**NST Code**"
      ],
      "metadata": {
        "id": "VzSsDDVTywXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image I/O helpers\n",
        "\n",
        "**`load_image(path, target_long_side=None)`**\n",
        "- **Force RGB (3 channels):** keeps shape consistent for VGG / PyTorch.\n",
        "- **Optional shrink (keep aspect ratio):** if `target_long_side` is set, scale the image so its **longer side = target_long_side**.  \n",
        "  *Why:* fewer pixels ⇒ faster, uses less memory. (Halving width & height ≈ **4× fewer pixels**.)\n",
        "- **High-quality resize (`Image.LANCZOS`):** when you shrink, many pixels are merged. A good filter **averages smartly** so diagonals don’t look like **stair-steps (“jaggies”)** and fine textures don’t turn into **wavy stripes**. Cleaner inputs ⇒ cleaner VGG features.\n",
        "- **To tensor in `[0,1]`:** `transforms.ToTensor()` gives a float tensor **(3, H, W)** scaled to `[0,1]`.\n",
        "- **Add batch dim:** `unsqueeze(0)` → **(1, 3, H, W)** because models expect a batch.\n",
        "- **Send to device:** `.to(device)` moves it to CPU or GPU.\n",
        "\n",
        "**`save_image(tensor, path)`**\n",
        "- **Stop tracking gradients:** `detach()` — we’re just saving numbers now.\n",
        "- **Keep valid range:** `clamp(0,1)` so colors aren’t out of bounds.\n",
        "- **Back to CPU & drop batch:** `cpu().squeeze(0)` → **(3, H, W)**.\n",
        "- **Write file:** `transforms.ToPILImage()(x).save(Path(path))`."
      ],
      "metadata": {
        "id": "dkidXX0v2GT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(path, target_long_side=None):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if target_long_side is not None:\n",
        "        w, h = img.size\n",
        "        scale = target_long_side / max(w, h)\n",
        "        img = img.resize((round(w*scale), round(h*scale)), Image.LANCZOS)\n",
        "    x = transforms.ToTensor()(img).unsqueeze(0).to(device)  # (1,3,H,W) in [0,1]\n",
        "    return x\n",
        "\n",
        "def save_image(tensor, path):\n",
        "    x = tensor.detach().clamp(0,1).cpu().squeeze(0)\n",
        "    transforms.ToPILImage()(x).save(Path(path))"
      ],
      "metadata": {
        "id": "jRGp7hskyxMP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalization module\n",
        "\n",
        "**What it does**\n",
        "- Takes an image batch `x` with shape **(N, 3, H, W)** in **[0,1]** and applies **per-channel** ImageNet normalization:  \n",
        "  `x_norm = (x − mean) / std`.\n",
        "- Stores `mean` and `std` as tensors shaped **(1, 3, 1, 1)** so they **broadcast** across batch and spatial dims (H×W).\n",
        "\n",
        "**Why we need it**\n",
        "- **VGG-19** was trained on ImageNet-normalized RGB. Feeding inputs in the same scale makes VGG features **comparable and stable** for style/content losses.\n",
        "- Using `register_buffer(...)` puts `mean/std` on the **right device**, includes them in the **state_dict**, and keeps them **non-trainable** (not updated by the optimizer).\n",
        "\n",
        "*Typical constants:* `mean = [0.485, 0.456, 0.406]`, `std = [0.229, 0.224, 0.225]`."
      ],
      "metadata": {
        "id": "xFfa0R0w30-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"mean\", mean.view(1,3,1,1))\n",
        "        self.register_buffer(\"std\",  std.view(1,3,1,1))\n",
        "    def forward(self, x): return (x - self.mean) / self.std"
      ],
      "metadata": {
        "id": "-3EUQpy81xTG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG “taps” — which layers we read and why\n",
        "\n",
        "**What is a tap?**  \n",
        "A *tap* is a layer in VGG where we **read** the activation (feature map) during the forward pass. VGG is **frozen** (not trained). We only use the tapped layers in our losses; **all other layers are ignored**.\n",
        "\n",
        "**Are we using all layers?**  \n",
        "**No.** We select a small subset:\n",
        "- **Content taps** → preserve the photo’s layout/structure.\n",
        "- **Style taps** → capture texture/brush/color statistics at multiple scales.\n",
        "\n",
        "**Our choices**\n",
        "```python\n",
        "# human-readable names for vgg19.features indices\n",
        "VGG_LAYER_NAMES = {1:\"relu1_1\", 6:\"relu2_1\", 11:\"relu3_1\", 20:\"relu4_1\", 22:\"relu4_2\", 29:\"relu5_1\"}\n",
        "\n",
        "# use this deep layer to keep object/layout structure\n",
        "CONTENT_LAYERS = [\"relu4_2\"]\n",
        "\n",
        "# use these (shallow→deep) to capture style across scales\n",
        "STYLE_LAYERS   = [\"relu1_1\",\"relu2_1\",\"relu3_1\",\"relu4_1\",\"relu5_1\"]"
      ],
      "metadata": {
        "id": "Z9pPOe0ENHMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG layer taps\n",
        "VGG_LAYER_NAMES = {1:\"relu1_1\", 6:\"relu2_1\", 11:\"relu3_1\", 20:\"relu4_1\", 22:\"relu4_2\", 29:\"relu5_1\"}\n",
        "CONTENT_LAYERS = [\"relu4_2\"]\n",
        "STYLE_LAYERS   = [\"relu1_1\",\"relu2_1\",\"relu3_1\",\"relu4_1\",\"relu5_1\"]"
      ],
      "metadata": {
        "id": "P_jDS4QFM86v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `VGGFeatures` — frozen VGG wrapper that returns only the layers we care about\n",
        "\n",
        "**Goal:** In one forward pass, (1) normalize the input exactly like ImageNet and (2) collect activations at selected “tap” layers for our losses.\n",
        "\n",
        "**How it’s built (`__init__`):**\n",
        "- Load `vgg19.features` (try the new weights API; fall back to `pretrained=True`).\n",
        "- Call `eval()` and set `requires_grad_(False)` → VGG is **frozen** (we never train it).\n",
        "- Create `Normalization(mean, std)` so inputs are ImageNet-normalized before entering VGG.\n",
        "\n",
        "**What `forward(x)` does:**\n",
        "1. Normalize: `x ← (x − mean) / std`.\n",
        "2. Run through VGG **layer by layer**.\n",
        "3. If the current layer index is listed in `VGG_LAYER_NAMES`, **store that activation** under its readable name (e.g., `\"relu4_2\"`).\n",
        "4. Return a dict mapping names → activations (each `(N, C, H, W)`), e.g.:\n",
        "   `{\"relu1_1\": T1, \"relu2_1\": T2, \"relu3_1\": T3, \"relu4_1\": T4, \"relu4_2\": T5, \"relu5_1\": T6}`.\n",
        "\n",
        "**Why this design:**\n",
        "- We do **not** use all VGG layers. We tap a small subset:\n",
        "  - **Content loss:** compare `X` vs `C` at a deep tap (e.g., `\"relu4_2\"`).\n",
        "  - **Style loss:** compare Gram stats at several shallow→deep taps.\n",
        "- Freezing VGG + correct normalization → features are **stable and comparable**; only your stylizer (or pixels) is updated."
      ],
      "metadata": {
        "id": "cVj4TJ4JR4hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGFeatures(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Compatibility with different torchvision versions\n",
        "        try:\n",
        "            feats = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_FEATURES).features\n",
        "        except Exception:\n",
        "            feats = models.vgg19(pretrained=True).features\n",
        "        self.vgg = feats.eval().to(device)\n",
        "        for p in self.vgg.parameters(): p.requires_grad_(False)\n",
        "        self.norm = Normalization(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    def forward(self, x):\n",
        "        out = {}\n",
        "        x = self.norm(x)\n",
        "        for i, layer in enumerate(self.vgg):\n",
        "            x = layer(x)\n",
        "            if i in VGG_LAYER_NAMES:\n",
        "                out[VGG_LAYER_NAMES[i]] = x\n",
        "        return out"
      ],
      "metadata": {
        "id": "xgIG6IlrRoun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precompute content & style targets — what this code does\n",
        "\n",
        "1. **Load images** (`C`, `S`) at a manageable size → tensors `(1,3,H,W)` on the correct device.\n",
        "2. **Frozen VGG taps**: build `VGGFeatures()` (it normalizes to ImageNet and returns only the tapped layers).\n",
        "3. **Forward once**: run `C` and `S` through VGG → get dictionaries of tapped activations: `c_feats`, `s_feats`.\n",
        "4. **Content target(s)**: take the content image activations at `CONTENT_LAYERS` (e.g., `relu4_2`). These preserve layout/structure.\n",
        "5. **Style targets**: for each layer in `STYLE_LAYERS`, compute a **Gram matrix** from the style activations:\n",
        "   - reshape features to `(C, N)` where `C`=channels and `N=H*W`;\n",
        "   - build a `(C×C)` matrix of **average channel-wise products** → captures texture/color statistics while ignoring exact positions.\n",
        "   - **Why Gram matrices at multiple style layers?**  \n",
        "Style is about which features co-occur, not where. A Gram matrix captures channel co-activation (second-order stats) and ignores spatial layout, so matching `G_l(X)` to `G_l(S)` transfers texture/brushwork without copying positions. Using several layers (shallow→deep) covers multi-scale style; normalizing by `H×W` keeps losses comparable across sizes.\n",
        "\n",
        "6. **Sanity check**: print shapes of targets to confirm they were created.\n",
        "\n",
        "**Use next:** feed `content_targets` and `style_targets` into your loss:\n",
        "- (a) **Optimize pixels** `X` directly, or\n",
        "- (b) **Train a stylizer network** `f_ϕ` so that `X = f_ϕ(C)`."
      ],
      "metadata": {
        "id": "ufwmKt2IB3Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CONTENT = \"/content/drive/MyDrive/nst/images/content.jpg\"\n",
        "STYLE   = \"/content/drive/MyDrive/nst/images/style.jpg\""
      ],
      "metadata": {
        "id": "EKhJ_w9PNFak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: precompute content/style targets with VGG taps\n",
        "# 1) Load images at a manageable size\n",
        "target_long_side = 512\n",
        "C = load_image(\"path/to/content.jpg\", target_long_side=target_long_side)  # (1,3,H,W) in [0,1]\n",
        "S = load_image(\"path/to/style.jpg\",   target_long_side=target_long_side)\n",
        "\n",
        "# 2) Frozen VGG feature extractor with your taps & normalization\n",
        "vgg = VGGFeatures()\n",
        "\n",
        "# 3) Forward once to get tapped activations\n",
        "with torch.no_grad():\n",
        "    c_feats = vgg(C)   # dict: {layer_name: activation}\n",
        "    s_feats = vgg(S)\n",
        "\n",
        "# 4) Build targets\n",
        "content_targets = {name: c_feats[name].detach() for name in CONTENT_LAYERS}\n",
        "\n",
        "def gram(feat):\n",
        "    # feat: (1, C, H, W) -> (C, C) average pairwise channel products\n",
        "    _, C, H, W = feat.shape\n",
        "    F = feat.view(C, H * W)         # (C, N)\n",
        "    return (F @ F.t()) / (H * W)    # (C, C)\n",
        "\n",
        "style_targets = {name: gram(s_feats[name]).detach() for name in STYLE_LAYERS}\n",
        "\n",
        "# 5) Sanity check\n",
        "print(\"Content taps:\")\n",
        "for k, v in content_targets.items():\n",
        "    print(f\"  {k}: {tuple(v.shape)}\")\n",
        "print(\"Style Gram targets:\")\n",
        "for k, G in style_targets.items():\n",
        "    print(f\"  {k}: {tuple(G.shape)}\")\n",
        "\n",
        "# content_targets and style_targets are now ready for the next step (optimize pixels X or train a stylizer f_phi)."
      ],
      "metadata": {
        "id": "RfoYcbM7BVBf",
        "outputId": "b3ed9f9a-317e-416c-f379-04dd3f0c492e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path/to/content.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-887067919.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1) Load images at a manageable size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_long_side\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path/to/content.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_long_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_long_side\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1,3,H,W) in [0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path/to/style.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mtarget_long_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_long_side\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-247690367.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(path, target_long_side)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_long_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_long_side\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_long_side\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/content.jpg'"
          ]
        }
      ]
    }
  ]
}