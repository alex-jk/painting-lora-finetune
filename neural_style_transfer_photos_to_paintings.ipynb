{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjgWlC1U0clrZX0qo/8ghh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/painting-lora-finetune/blob/main/neural_style_transfer_photos_to_paintings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Neural style transfer — brief**\n",
        "**Goal**<br>\n",
        "Turn your photo into a painting-like image **without moving objects**: keep the photo’s layout, borrow the painting’s colors/texture.\n",
        "\n",
        "**How it works (high level)**<br>\n",
        "We create an output image `X` and **optimize its pixels** so that:<br>\n",
        "• its **deep features** (from a CNN) match the photo → preserves structure/content;<br>\n",
        "• its **feature statistics** (Gram matrices) match the painting → transfers style/brushwork.\n",
        "\n",
        "**VGG-19**<br>\n",
        "**VGG-19** is a classic **convolutional neural network (CNN)** trained on ImageNet. It stacks many conv layers that detect edges, textures, parts, and objects.<br>\n",
        "• **Shallow layers** respond to color/texture; **deep layers** to object/layout.<br>\n",
        "• We **freeze** VGG (no training) and use it only to extract features that act as our “perceptual rulers.”<br>\n",
        "• Matching **deep** VGG features → keeps object placements. Matching **Gram stats** across layers → injects painting style.\n",
        "\n",
        "**Pipeline**<br>\n",
        "Load & normalize images → run through frozen VGG → compute **content loss** (deep layer), **style loss** (Gram matrices across several layers), plus tiny **TV** smoothing → backprop gradients **to the image pixels** with L-BFGS/Adam until it looks right.\n",
        "\n",
        "**Total Variation (TV) smoothing** - A regularizer that penalizes rapid pixel-to-pixel changes. It prefers images that are locally smooth (piecewise-smooth) without simply blurring everything."
      ],
      "metadata": {
        "id": "-0NeC4NdFWWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "JkuuncyXyG4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6LSFUnVPsgd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf651b84-aa58-4585-b979-f06e19219b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 | Vision: 0.23.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Colab usually has recent torch/torchvision, but this is safe.\n",
        "!pip -q install --upgrade torch torchvision pillow\n",
        "import torch, torchvision, PIL\n",
        "print(\"Torch:\", torch.__version__, \"| Vision:\", torchvision.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "WSqes22jy00u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Set device and ImageNet normalization (for VGG-19 features)**\n",
        "\n",
        "- **Device selection:** Use GPU (`cuda`) if available; otherwise fall back to CPU. All tensors/ops must be on the **same device**.\n",
        "- **ImageNet normalization:** VGG-19 expects RGB inputs scaled to **[0,1]** and normalized per channel with:\n",
        "  - mean = `[0.485, 0.456, 0.406]`\n",
        "  - std  = `[0.229, 0.224, 0.225]`\n",
        "<br>We apply it as: `(x - mean) / std` (broadcast per channel over H×W).\n",
        "- **Why:** Feeding the exact normalization used in training keeps VGG feature distributions correct; skipping it can cause unstable optimization or odd colors.\n",
        "- **`.to(device)` on mean/std:** Puts these constants on the same device as your images to avoid device-mismatch errors."
      ],
      "metadata": {
        "id": "8OG7pKx2zG8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).to(device)"
      ],
      "metadata": {
        "id": "8Alx0zfjy47k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**NST Code**"
      ],
      "metadata": {
        "id": "VzSsDDVTywXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image I/O helpers\n",
        "\n",
        "**load_image(path, target_long_side=None)**<br>\n",
        "Convert to **RGB** so every image is 3 channels as VGG expects.<br>\n",
        "Resize by the **longer side** to control compute/memory while keeping aspect ratio; **LANCZOS** gives high-quality downscaling (less aliasing → cleaner features).<br>\n",
        "`ToTensor()` makes a float tensor scaled to **[0,1]**, which is required for the ImageNet normalization we apply later.<br>\n",
        "`unsqueeze(0)` adds a **batch dimension** → models expect shape `(N, C, H, W)` even for a single image.<br>\n",
        "\n",
        "**save_image(tensor, path)**<br>\n",
        "`detach()` drops the autograd graph since we’re done optimizing pixels and just want the data.<br>\n",
        "`clamp(0,1)` enforces valid image range after optimization so saved colors are not out of bounds.<br>\n",
        "`cpu()` moves data to CPU because PIL saves from CPU tensors/arrays.<br>\n",
        "`squeeze(0)` removes the batch dimension; PIL expects `(C, H, W)` (or `(H, W, C)` after conversion).<br>"
      ],
      "metadata": {
        "id": "dkidXX0v2GT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(path, target_long_side=None):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if target_long_side is not None:\n",
        "        w, h = img.size\n",
        "        scale = target_long_side / max(w, h)\n",
        "        img = img.resize((round(w*scale), round(h*scale)), Image.LANCZOS)\n",
        "    x = transforms.ToTensor()(img).unsqueeze(0).to(device)  # (1,3,H,W) in [0,1]\n",
        "    return x\n",
        "\n",
        "def save_image(tensor, path):\n",
        "    x = tensor.detach().clamp(0,1).cpu().squeeze(0)\n",
        "    transforms.ToPILImage()(x).save(Path(path))"
      ],
      "metadata": {
        "id": "jRGp7hskyxMP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalization module\n",
        "\n",
        "**What it does**<br>\n",
        "Applies per-channel ImageNet normalization to an input tensor: `(x - mean) / std`.<br>\n",
        "Stores `mean` and `std` reshaped to `(1, 3, 1, 1)` so they broadcast over H×W.\n",
        "\n",
        "**Why we need it**<br>\n",
        "VGG-19 was trained on ImageNet-normalized RGB; matching that distribution makes its features meaningful and stable for NST.<br>\n",
        "`register_buffer(...)` keeps `mean/std` on the right device (move with `.to(device)`), included in `state_dict`, and **not** trainable parameters."
      ],
      "metadata": {
        "id": "xFfa0R0w30-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"mean\", mean.view(1,3,1,1))\n",
        "        self.register_buffer(\"std\",  std.view(1,3,1,1))\n",
        "    def forward(self, x): return (x - self.mean) / self.std"
      ],
      "metadata": {
        "id": "-3EUQpy81xTG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG layer “taps”\n",
        "\n",
        "A *tap* is a read-only checkpoint inside VGG where we capture that layer’s activation (feature map) during the forward pass. VGG stays frozen; we only **read** these tensors.\n",
        "\n",
        "**`VGG_LAYER_NAMES`**<br>\n",
        "Maps raw indices in `vgg19.features` to human-readable names (e.g., index 1 → `relu1_1`). This tells the code exactly which intermediate outputs to capture.\n",
        "\n",
        "**`CONTENT_LAYERS = [\"relu4_2\"]`**<br>\n",
        "Deep feature(s) used for the **content loss** so the output keeps the photo’s object/layout structure.\n",
        "\n",
        "**`STYLE_LAYERS = [\"relu1_1\",\"relu2_1\",\"relu3_1\",\"relu4_1\",\"relu5_1\"]`**<br>\n",
        "Shallow→deep features used for the **style loss**. We form Gram matrices at each to capture multi-scale texture/brush/color statistics.\n",
        "\n",
        "**Why these choices**<br>\n",
        "Shallow layers encode edges/colors; deeper layers encode parts/layout. Using one deep content layer plus several style layers yields strong layout preservation with rich style transfer.\n"
      ],
      "metadata": {
        "id": "Z9pPOe0ENHMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG layer taps\n",
        "VGG_LAYER_NAMES = {1:\"relu1_1\", 6:\"relu2_1\", 11:\"relu3_1\", 20:\"relu4_1\", 22:\"relu4_2\", 29:\"relu5_1\"}\n",
        "CONTENT_LAYERS = [\"relu4_2\"]\n",
        "STYLE_LAYERS   = [\"relu1_1\",\"relu2_1\",\"relu3_1\",\"relu4_1\",\"relu5_1\"]"
      ],
      "metadata": {
        "id": "P_jDS4QFM86v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `VGGFeatures` — frozen VGG wrapper that returns tapped activations\n",
        "\n",
        "**Purpose**<br>\n",
        "Wrap a pretrained VGG-19 so we can **normalize inputs** and **capture specific layer outputs** (our taps) in one forward pass.\n",
        "\n",
        "**Init (`__init__`)**<br>\n",
        "- Loads VGG-19 features (with a fallback for older torchvision).<br>\n",
        "- `eval()` puts VGG in inference mode (no dropout/bn updates).<br>\n",
        "- Freezes weights: `requires_grad_(False)` so we only optimize pixels, not VGG.<br>\n",
        "- Creates `Normalization(mean,std)` to apply ImageNet normalization.\n",
        "\n",
        "**Forward (`forward`)**<br>\n",
        "- Normalizes input `x` to match VGG’s training distribution.<br>\n",
        "- Iterates through VGG layers; after each layer, if its index is in `VGG_LAYER_NAMES`, **stores the activation** in a dict under its human-readable name (e.g., `relu4_2`).<br>\n",
        "- Returns a dict `{layer_name: activation}` used to compute **content** and **style** losses."
      ],
      "metadata": {
        "id": "cVj4TJ4JR4hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGFeatures(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Compatibility with different torchvision versions\n",
        "        try:\n",
        "            feats = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_FEATURES).features\n",
        "        except Exception:\n",
        "            feats = models.vgg19(pretrained=True).features\n",
        "        self.vgg = feats.eval().to(device)\n",
        "        for p in self.vgg.parameters(): p.requires_grad_(False)\n",
        "        self.norm = Normalization(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    def forward(self, x):\n",
        "        out = {}\n",
        "        x = self.norm(x)\n",
        "        for i, layer in enumerate(self.vgg):\n",
        "            x = layer(x)\n",
        "            if i in VGG_LAYER_NAMES:\n",
        "                out[VGG_LAYER_NAMES[i]] = x\n",
        "        return out"
      ],
      "metadata": {
        "id": "xgIG6IlrRoun"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}