{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoeDAOYj6TkF0rJlwISiLM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/painting-lora-finetune/blob/main/neural_style_transfer_photos_to_paintings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set before importing torch; then Runtime -> Restart runtime\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ],
      "metadata": {
        "id": "a6RMrjh2DzZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font color=\"#0b3d91\">Neural Style Transfer Method</font></h3>\n",
        "\n",
        "**Goal**\n",
        "\n",
        "Turn your photo into a painting-like image **without moving objects**: keep the photo’s layout, borrow the painting’s colors/texture.\n",
        "\n",
        "**How it works (high level)**<br>\n",
        "We create an output image `X` and **optimize its pixels** so that:<br>\n",
        "• its **deep features** (from a CNN) match the photo → preserves structure/content;<br>\n",
        "• its **feature statistics** (Gram matrices) match the painting → transfers style/brushwork.\n",
        "\n",
        "**VGG-19**<br>\n",
        "**VGG-19** is a classic **convolutional neural network (CNN)** trained on ImageNet. It stacks many conv layers that detect edges, textures, parts, and objects.<br>\n",
        "• **Shallow layers** respond to color/texture; **deep layers** to object/layout.<br>\n",
        "• We **freeze** VGG (no training) and use it only to extract features that act as our “perceptual rulers.”<br>\n",
        "• To preserve the photo's content, we match its core VGG features. To inject the painting's style, we match its Gram stats, which are a statistical measurement of which textures and patterns (like certain brushstrokes and colors) tend to appear together throughout the image.\n",
        "\n",
        "<h3><font color=\"#0b3d91\">How the neural style transfer model is trained (fast/feed-forward version)</font></h3>\n",
        "\n",
        "**Notation**\n",
        "- `f_phi`: trainable **stylizer** CNN (the only network you update)\n",
        "- `VGG`: frozen **loss network** (used only to compute losses)\n",
        "- `C`: content image, `S`: style image\n",
        "- `X = f_phi(C)`: stylized output\n",
        "\n",
        "**Training loop**\n",
        "1. **Freeze VGG.** Use it only to extract features for the losses (no weight updates).\n",
        "2. **Choose a style** image `S` (or sample from a style set).\n",
        "3. **For each content image `C`:**\n",
        "    - **Forward Pass:** We feed the content photo `C` through our trainable stylizer CNN `f_phi` to produce a candidate image `X`. This network is the one we are actively training, and its output is our `stylized guess`, which we will immediately use to measure how well we are matching our style and content goals.\n",
        "    - **Extract features (with frozen VGG):** get feature maps `F_l(C)`, `F_l(X)`, and `F_l(S)` from selected layers `l`.\n",
        "    - **Content loss:** This is the penalty for changing the photo's core content. We compute it by taking the feature representations of our stylized image `F_l(X)` and the original photo `F_l(C)` from a deeper VGG layer and calculating the squared Euclidean distance (or Mean Squared Error) between them. A smaller distance means the main objects in our guess still look like the original.\n",
        "    - **Style loss:** make **Gram matrices** of `X` match those of `S` across chosen layers (captures texture/brushwork).\n",
        "    - **TV loss (optional):** This is a penalty added to make the final image smoother and less noisy. The actual penalty is the `Total Variation` of the image `X`, which is calculated by summing the differences between each pixel and its immediate neighbors, both horizontally and vertically. A high value means the image has a lot of sharp, disconnected pixel noise, and minimizing it creates more gradual transitions.\n",
        "    - **Total loss:** `L = lambda_c * L_content + lambda_s * L_style + lambda_tv * L_tv`.\n",
        "    - **Backprop:** send gradients through VGG into `f_phi` (VGG stays frozen) and **update only `phi`** (e.g., Adam).\n",
        "4. **Repeat** over batches until validation loss plateaus. The trained `f_phi` then stylizes any new `C` in a single forward pass.\n",
        "\n",
        "> **Alternative (optimization-based NST):** don’t learn `f_phi`. Initialize `X` (e.g., `X <- C`) and **optimize the pixels of `X`** directly to minimize the same losses with VGG frozen.\n",
        "\n",
        "\n",
        "**Pipeline**<br>\n",
        "Load & normalize images → run through frozen VGG → compute **content loss** (deep layer), **style loss** (Gram matrices across several layers), plus tiny **total variation (TV)** smoothing → backprop gradients **to the image pixels** with L-BFGS/Adam until it looks right.\n",
        "\n",
        "**Total Variation (TV) smoothing** - A regularizer that penalizes rapid pixel-to-pixel changes. It prefers images that are locally smooth (piecewise-smooth) without simply blurring everything."
      ],
      "metadata": {
        "id": "-0NeC4NdFWWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font color=\"#0b3d91\">Gram matrices (style loss)</font></h3>\n",
        "\n",
        "**Setup (for one VGG layer `l`)**\n",
        "\n",
        "To transfer an artistic style, we need a way to measure 'style' itself. A simple feature match isn't enough, because we don't want to copy the objects from the style painting, just its texture and feel. Gram matrices are the solution. They are a mathematical tool that describes an image's style by ignoring object placement and instead capturing which visual patterns (like colors and brushstrokes) tend to appear together. By forcing our new image to have the same Gram matrix as the style painting, we force it to adopt the same artistic signature.\n",
        "\n",
        "- **Get Feature Maps from an Image:**\n",
        "First, we need to extract visual features from an image. Let's take our current version of the output image `X` as the example. We feed it through a specific layer `l` of the VGG network.\n",
        "The output, `F_l(X)`, is a set of \"feature maps\": This is a 3D block of data with shape `[C, H, W]`, representing what the VGG layer \"sees\" in our generated image.\n",
        "- **Flatten the Feature Maps:**\n",
        "The next step is to prepare this 3D block of data for the style calculation by collapsing its spatial dimensions `the height H and width W`. We reshape the feature maps `F_l(X)` into a new 2D matrix, `F`.\n",
        "  - This `F` matrix now has a shape of `[C, H*W]`. Each of the `C` rows represents a different feature channel, and the columns represent all the pixel locations. This transformation is essential because it organizes the data so we can easily measure the relationships between the different feature channels, which is the key to capturing style.\n",
        "- **Gram matrix (per layer):**\n",
        "  - Start with features `F_l(X)` of shape `[C, H, W]`.\n",
        "  - Flatten spatial dims → `F` has shape `[C, N]`, where `N = H * W`.\n",
        "  - Indices: `i, j ∈ {1..C}` are **channel indices**; `p ∈ {1..N}` is the **pixel (spatial) index** after flattening.\n",
        "  - Define the `[C, C]` matrix `G` by  \n",
        "    `G[i, j] = (1/N) * sum_{p=1..N} F[i, p] * F[j, p]`.\n",
        "  - This is the **average product across all spatial positions** of channels `i` and `j`.  \n",
        "    Averaging over `p` removes location and keeps **co-activation (texture) statistics**.\n",
        "\n",
        "**Intuition**\n",
        "- Entry `G[i, j]` is the dot product between channel `i` and `j` across all pixels → how strongly they **co-activate**.\n",
        "- Averaging over locations discards exact positions, so Gram matrices capture **texture/brushwork statistics** (which features occur together), not layout.\n",
        "- Matching `G_l(X)` to `G_l(S)` makes `X` use the same color/texture patterns as the style image at that layer’s scale.\n",
        "\n",
        "**Why multiple layers?**\n",
        "- Shallow layers → fine textures/colors; deeper layers → broader patterns.\n",
        "- Summing style losses over several layers gives a **multi-scale** style match.\n",
        "\n",
        "**Style loss formula**\n",
        "- Per layer: `L_style_l = || G_l(X) - G_l(S) ||_F^2`\n",
        "- Total style loss: weighted sum over chosen layers.\n",
        "- Normalize by `H*W` (or `C*H*W`) to keep values scale-stable."
      ],
      "metadata": {
        "id": "LnNbZSB3Br4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "JkuuncyXyG4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "6LSFUnVPsgd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e679445e-51c5-4aaa-f36d-0b3705cb6bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 | Vision: 0.23.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch, torchvision, PIL\n",
        "print(\"Torch:\", torch.__version__, \"| Vision:\", torchvision.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import zipfile, re, urllib.request"
      ],
      "metadata": {
        "id": "WSqes22jy00u"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Set device and ImageNet normalization (for VGG-19 features)**\n",
        "\n",
        "- **Device selection:** Use GPU (`cuda`) if available; otherwise fall back to CPU. All tensors/ops must be on the **same device**.\n",
        "- **ImageNet normalization:** VGG-19 expects RGB inputs scaled to **[0,1]** and normalized per channel with:\n",
        "  - mean = `[0.485, 0.456, 0.406]`\n",
        "  - std  = `[0.229, 0.224, 0.225]`\n",
        "<br>We apply it as: `(x - mean) / std` (broadcast per channel over H×W).\n",
        "- **Why:** Feeding the exact normalization used in training keeps VGG feature distributions correct; skipping it can cause unstable optimization or odd colors.\n",
        "- **`.to(device)` on mean/std:** Puts these constants on the same device as your images to avoid device-mismatch errors."
      ],
      "metadata": {
        "id": "8OG7pKx2zG8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).to(device)"
      ],
      "metadata": {
        "id": "8Alx0zfjy47k"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**NST Code**"
      ],
      "metadata": {
        "id": "VzSsDDVTywXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image I/O helpers\n",
        "\n",
        "**`load_image(path, target_long_side=None)`**\n",
        "- **Force RGB (3 channels):** keeps shape consistent for VGG / PyTorch.\n",
        "- **Optional shrink (keep aspect ratio):** if `target_long_side` is set, scale the image so its **longer side = target_long_side**.  \n",
        "  *Why:* fewer pixels ⇒ faster, uses less memory. (Halving width & height ≈ **4× fewer pixels**.)\n",
        "- **High-quality resize (`Image.LANCZOS`):** when you shrink, many pixels are merged. A good filter **averages smartly** so diagonals don’t look like **stair-steps (“jaggies”)** and fine textures don’t turn into **wavy stripes**. Cleaner inputs ⇒ cleaner VGG features.\n",
        "- **To tensor in `[0,1]`:** `transforms.ToTensor()` gives a float tensor **(3, H, W)** scaled to `[0,1]`.\n",
        "- **Add batch dim:** `unsqueeze(0)` → **(1, 3, H, W)** because models expect a batch.\n",
        "- **Send to device:** `.to(device)` moves it to CPU or GPU.\n",
        "\n",
        "**`save_image(tensor, path)`**\n",
        "- **Stop tracking gradients:** `detach()` — we’re just saving numbers now.\n",
        "- **Keep valid range:** `clamp(0,1)` so colors aren’t out of bounds.\n",
        "- **Back to CPU & drop batch:** `cpu().squeeze(0)` → **(3, H, W)**.\n",
        "- **Write file:** `transforms.ToPILImage()(x).save(Path(path))`."
      ],
      "metadata": {
        "id": "dkidXX0v2GT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(path, target_long_side=None):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if target_long_side is not None:\n",
        "        w, h = img.size\n",
        "        scale = target_long_side / max(w, h)\n",
        "        img = img.resize((round(w*scale), round(h*scale)), Image.LANCZOS)\n",
        "    x = transforms.ToTensor()(img).unsqueeze(0).to(device)  # (1,3,H,W) in [0,1]\n",
        "    return x\n",
        "\n",
        "def save_image(tensor, path):\n",
        "    x = tensor.detach().clamp(0,1).cpu().squeeze(0)\n",
        "    transforms.ToPILImage()(x).save(Path(path))"
      ],
      "metadata": {
        "id": "jRGp7hskyxMP"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalization module\n",
        "\n",
        "**What it does**\n",
        "- Takes an image batch `x` with shape **(N, 3, H, W)** in **[0,1]** and applies **per-channel** ImageNet normalization:  \n",
        "  `x_norm = (x − mean) / std`.\n",
        "- Stores `mean` and `std` as tensors shaped **(1, 3, 1, 1)** so they **broadcast** across batch and spatial dims (H×W).\n",
        "\n",
        "**Why we need it**\n",
        "- **VGG-19** was trained on ImageNet-normalized RGB. Feeding inputs in the same scale makes VGG features **comparable and stable** for style/content losses.\n",
        "- Using `register_buffer(...)` puts `mean/std` on the **right device**, includes them in the **state_dict**, and keeps them **non-trainable** (not updated by the optimizer).\n",
        "\n",
        "*Typical constants:* `mean = [0.485, 0.456, 0.406]`, `std = [0.229, 0.224, 0.225]`."
      ],
      "metadata": {
        "id": "xFfa0R0w30-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"mean\", mean.view(1,3,1,1))\n",
        "        self.register_buffer(\"std\",  std.view(1,3,1,1))\n",
        "    def forward(self, x): return (x - self.mean) / self.std"
      ],
      "metadata": {
        "id": "-3EUQpy81xTG"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG “taps” — which layers we read and why\n",
        "\n",
        "**What is a tap?**  \n",
        "A *tap* is a layer in VGG where we **read** the activation (feature map) during the forward pass. VGG is **frozen** (not trained). We only use the tapped layers in our losses; **all other layers are ignored**.\n",
        "\n",
        "**Are we using all layers?**  \n",
        "**No.** We select a small subset:\n",
        "- **Content taps** → preserve the photo’s layout/structure.\n",
        "- **Style taps** → capture texture/brush/color statistics at multiple scales.\n",
        "\n",
        "**Our choices**\n",
        "```python\n",
        "# human-readable names for vgg19.features indices\n",
        "VGG_LAYER_NAMES = {1:\"relu1_1\", 6:\"relu2_1\", 11:\"relu3_1\", 20:\"relu4_1\", 22:\"relu4_2\", 29:\"relu5_1\"}\n",
        "\n",
        "# use this deep layer to keep object/layout structure\n",
        "CONTENT_LAYERS = [\"relu4_2\"]\n",
        "\n",
        "# use these (shallow→deep) to capture style across scales\n",
        "STYLE_LAYERS   = [\"relu1_1\",\"relu2_1\",\"relu3_1\",\"relu4_1\",\"relu5_1\"]"
      ],
      "metadata": {
        "id": "Z9pPOe0ENHMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG layer taps\n",
        "VGG_LAYER_NAMES = {1:\"relu1_1\", 6:\"relu2_1\", 11:\"relu3_1\", 20:\"relu4_1\", 22:\"relu4_2\", 29:\"relu5_1\"}\n",
        "CONTENT_LAYERS = [\"relu4_2\"]\n",
        "STYLE_LAYERS   = [\"relu1_1\",\"relu2_1\",\"relu3_1\",\"relu4_1\",\"relu5_1\"]"
      ],
      "metadata": {
        "id": "P_jDS4QFM86v"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `VGGFeatures` — frozen VGG wrapper that returns only the layers we care about\n",
        "\n",
        "**Goal:** In one forward pass, (1) normalize the input exactly like ImageNet and (2) collect activations at selected “tap” layers for our losses.\n",
        "\n",
        "**How it’s built (`__init__`):**\n",
        "- Load `vgg19.features` (try the new weights API; fall back to `pretrained=True`).\n",
        "- Call `eval()` and set `requires_grad_(False)` → VGG is **frozen** (we never train it).\n",
        "- Create `Normalization(mean, std)` so inputs are ImageNet-normalized before entering VGG.\n",
        "\n",
        "**What `forward(x)` does:**\n",
        "1. Normalize: `x ← (x − mean) / std`.\n",
        "2. Run through VGG **layer by layer**.\n",
        "3. If the current layer index is listed in `VGG_LAYER_NAMES`, **store that activation** under its readable name (e.g., `\"relu4_2\"`).\n",
        "4. Return a dict mapping names → activations (each `(N, C, H, W)`), e.g.:\n",
        "   `{\"relu1_1\": T1, \"relu2_1\": T2, \"relu3_1\": T3, \"relu4_1\": T4, \"relu4_2\": T5, \"relu5_1\": T6}`.\n",
        "\n",
        "**Why this design:**\n",
        "- We do **not** use all VGG layers. We tap a small subset:\n",
        "  - **Content loss:** compare `X` vs `C` at a deep tap (e.g., `\"relu4_2\"`).\n",
        "  - **Style loss:** compare Gram stats at several shallow→deep taps.\n",
        "- Freezing VGG + correct normalization → features are **stable and comparable**; only your stylizer (or pixels) is updated."
      ],
      "metadata": {
        "id": "cVj4TJ4JR4hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGFeatures(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Compatibility with different torchvision versions\n",
        "        try:\n",
        "            feats = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_FEATURES).features\n",
        "        except Exception:\n",
        "            feats = models.vgg19(pretrained=True).features\n",
        "        self.vgg = feats.eval().to(device)\n",
        "        for p in self.vgg.parameters(): p.requires_grad_(False)\n",
        "        self.norm = Normalization(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    def forward(self, x):\n",
        "        out = {}\n",
        "        x = self.norm(x)\n",
        "        for i, layer in enumerate(self.vgg):\n",
        "            x = layer(x)\n",
        "            if i in VGG_LAYER_NAMES:\n",
        "                out[VGG_LAYER_NAMES[i]] = x\n",
        "        return out"
      ],
      "metadata": {
        "id": "xgIG6IlrRoun"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precompute content & style targets — what this code does\n",
        "\n",
        "1. **Load images** (`C`, `S`) at a manageable size → tensors `(1,3,H,W)` on the correct device.\n",
        "2. **Frozen VGG taps**: build `VGGFeatures()` (it normalizes to ImageNet and returns only the tapped layers).\n",
        "3. **Forward once**: run `C` and `S` through VGG → get dictionaries of tapped activations: `c_feats`, `s_feats`.\n",
        "4. **Content target(s)**: take the content image activations at `CONTENT_LAYERS` (e.g., `relu4_2`). These preserve layout/structure.\n",
        "5. **Style targets**: for each layer in `STYLE_LAYERS`, compute a **Gram matrix** from the style activations:\n",
        "   - reshape features to `(C, N)` where `C`=channels and `N=H*W`;\n",
        "   - build a `(C×C)` matrix of **average channel-wise products** → captures texture/color statistics while ignoring exact positions.\n",
        "   - **Why Gram matrices at multiple style layers?**  \n",
        "Style is about which features co-occur, not where. A Gram matrix captures channel co-activation (second-order stats) and ignores spatial layout, so matching `G_l(X)` to `G_l(S)` transfers texture/brushwork without copying positions. Using several layers (shallow→deep) covers multi-scale style; normalizing by `H×W` keeps losses comparable across sizes.\n",
        "\n",
        "6. **Sanity check**: print shapes of targets to confirm they were created.\n",
        "\n",
        "**Use next:** feed `content_targets` and `style_targets` into your loss:\n",
        "- (a) **Optimize pixels** `X` directly, or\n",
        "- (b) **Train a stylizer network** `f_ϕ` so that `X = f_ϕ(C)`."
      ],
      "metadata": {
        "id": "ufwmKt2IB3Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CONTENT = \"/content/drive/MyDrive/nst/images/content.jpg\"\n",
        "STYLE   = \"/content/drive/MyDrive/nst/images/style.jpg\""
      ],
      "metadata": {
        "id": "EKhJ_w9PNFak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fde2e4d-ff61-4fa9-d1e0-904cce389866"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: precompute content/style targets with VGG taps\n",
        "# 1) Load images at a manageable size\n",
        "# target_long_side = 512\n",
        "# C = load_image(CONTENT, target_long_side=target_long_side)  # was \"path/to/content.jpg\"\n",
        "# S = load_image(STYLE,   target_long_side=target_long_side)  # was \"path/to/style.jpg\"\n",
        "\n",
        "# pick your print size first: e.g., 16×20\" @ 240 PPI → long side = 4800 px (portrait)\n",
        "CONTENT_LONG = 4800          # your phone photo, full-resolution target\n",
        "STYLE_LONG   = 512           # style image can stay small\n",
        "\n",
        "C = load_image(CONTENT, target_long_side=CONTENT_LONG)  # no 512 cap anymore\n",
        "S = load_image(STYLE,   target_long_side=STYLE_LONG)\n",
        "\n",
        "# 2) Frozen VGG feature extractor with your taps & normalization\n",
        "vgg = VGGFeatures()\n",
        "\n",
        "# 3) Forward once to get tapped activations\n",
        "with torch.no_grad():\n",
        "    c_feats = vgg(C)   # dict: {layer_name: activation}\n",
        "    s_feats = vgg(S)\n",
        "\n",
        "# 4) Build targets\n",
        "content_targets = {name: c_feats[name].detach() for name in CONTENT_LAYERS}\n",
        "\n",
        "def gram(feat):\n",
        "    # feat: (1, C, H, W) -> (C, C) average pairwise channel products\n",
        "    _, C, H, W = feat.shape\n",
        "    F = feat.view(C, H * W)         # (C, N)\n",
        "    return (F @ F.t()) / (H * W)    # (C, C)\n",
        "\n",
        "style_targets = {name: gram(s_feats[name]).detach() for name in STYLE_LAYERS}\n",
        "\n",
        "# 5) Sanity check\n",
        "print(\"Content taps:\")\n",
        "for k, v in content_targets.items():\n",
        "    print(f\"  {k}: {tuple(v.shape)}\")\n",
        "print(\"Style Gram targets:\")\n",
        "for k, G in style_targets.items():\n",
        "    print(f\"  {k}: {tuple(G.shape)}\")\n",
        "\n",
        "# content_targets and style_targets are now ready for the next step (optimize pixels X or train a stylizer f_phi)."
      ],
      "metadata": {
        "id": "RfoYcbM7BVBf",
        "outputId": "8c183d60-7a03-466d-b170-408401d37937",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content taps:\n",
            "  relu4_2: (1, 512, 600, 451)\n",
            "Style Gram targets:\n",
            "  relu1_1: (64, 64)\n",
            "  relu2_1: (128, 128)\n",
            "  relu3_1: (256, 256)\n",
            "  relu4_1: (512, 512)\n",
            "  relu5_1: (512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize X (stylized image) using your precomputed targets, then save it.\n",
        "\n",
        "steps, lr = 300, 0.02\n",
        "w_content, w_style, w_tv = 1.0, 10.0, 1e-5\n",
        "\n",
        "def total_variation(x):\n",
        "    tv_h = (x[:, :, 1:, :] - x[:, :, :-1, :]).abs().mean()\n",
        "    tv_w = (x[:, :, :, 1:] - x[:, :, :, :-1]).abs().mean()\n",
        "    return tv_h + tv_w\n",
        "\n",
        "X = C.clone().requires_grad_(True)\n",
        "opt = torch.optim.Adam([X], lr=lr)\n",
        "\n",
        "for t in range(1, steps + 1):\n",
        "    opt.zero_grad()\n",
        "    feats = vgg(X)\n",
        "\n",
        "    # content loss (keep layout/structure)\n",
        "    Lc = sum(torch.nn.functional.mse_loss(feats[n], content_targets[n]) for n in CONTENT_LAYERS)\n",
        "\n",
        "    # style loss (match Gram stats)\n",
        "    Ls = 0.0\n",
        "    for n in STYLE_LAYERS:\n",
        "        Ls += torch.nn.functional.mse_loss(gram(feats[n]), style_targets[n])\n",
        "\n",
        "    # smoothness\n",
        "    Ltv = total_variation(X)\n",
        "\n",
        "    loss = w_content*Lc + w_style*Ls + w_tv*Ltv\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        X.clamp_(0, 1)\n",
        "\n",
        "    if t % 50 == 0 or t == steps:\n",
        "        print(f\"[{t}/{steps}] Lc={Lc.item():.4f}  Ls={Ls.item():.4f}  Ltv={Ltv.item():.6f}  Total={loss.item():.4f}\")\n",
        "\n",
        "save_image(X, \"/content/out.png\")\n",
        "print(\"Saved: /content/out.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Gow8HMVbf-9H",
        "outputId": "a104656f-40ee-413b-b644-ef2e21f160d1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 4.14 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.21 GiB is free. Process 13358 has 12.53 GiB memory in use. Of the allocated memory 9.26 GiB is allocated by PyTorch, and 3.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1303963269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# content loss (keep layout/structure)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2815259028.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mVGG_LAYER_NAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVGG_LAYER_NAMES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.14 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.21 GiB is free. Process 13358 has 12.53 GiB memory in use. Of the allocated memory 9.26 GiB is allocated by PyTorch, and 3.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "display(Image.open(\"/content/out.png\"))"
      ],
      "metadata": {
        "id": "BeIK6gwogldx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Painterly stylization\n",
        "\n",
        "**Goal**  \n",
        "Create a stylized image that looks more like a painting: keep the overall composition while reducing fine photo detail and “sand” noise.\n",
        "\n",
        "---\n",
        "\n",
        "#### Hyperparameters\n",
        "- `steps = 500` — number of optimization updates.\n",
        "- `w_content = 1.0` — weight for **content loss** (keeps global structure).\n",
        "- `w_style = 12.0` — weight for **style loss** (drives brush/texture/color).\n",
        "- `w_tv = 5e-5` — weight for **total variation loss** (adds smoothness; suppresses speckle).\n",
        "\n",
        "*Effect:* increase `w_style` → stronger style; increase `w_content` → more structure; increase `w_tv` → smoother, less noisy.\n",
        "\n",
        "---\n",
        "\n",
        "#### Style layers (which “style” features to match)\n",
        "- We emphasize **mid/deep taps** (`relu3_1`, `relu4_1`) and down-weight **shallow taps** (`relu1_1`, `relu2_1`, `relu5_1`).\n",
        "- *Why:* shallow layers tend to produce very fine, high-frequency textures; mid/deep taps encourage broader, painterly patterns.\n",
        "\n",
        "---\n",
        "\n",
        "#### Losses computed each step\n",
        "- **Content loss (`Lc`)**: MSE between VGG features of the current image `X` and the content image at `CONTENT_LAYERS` → preserves overall arrangement.\n",
        "- **Style loss (`Ls`)**: MSE between **Gram matrices** of `X` and the style image at `STYLE_LAYERS`, scaled by the per-layer weights → transfers multi-scale texture/brush/color statistics.\n",
        "- **Total variation (`Ltv`)**: encourages neighboring pixels to be similar → reduces pixel-level noise.\n",
        "\n",
        "---\n",
        "\n",
        "#### Gaussian blur helper (`gauss_blur`)\n",
        "- Builds a 1D Gaussian kernel (size `k`, std `sigma`), normalizes it, and applies **two separable depthwise convolutions** (vertical then horizontal) per RGB channel.\n",
        "- *Why used here:* as a light **low-pass filter** to tame high-frequency artifacts without destroying larger shapes.\n",
        "\n",
        "---\n",
        "\n",
        "#### Initialization\n",
        "- Start `X` from a **mildly blurred** version of the content image plus **tiny noise**.\n",
        "- *Why:* retains composition but softens sharp photo edges, making it easier to form paint-like regions rather than copying photographic detail.\n",
        "\n",
        "---\n",
        "\n",
        "#### Optimization loop (what happens each iteration)\n",
        "1. Run `X` through **frozen VGG** to get tapped features.\n",
        "2. Compute `Lc`, `Ls` (with the style layer weights), and `Ltv`.\n",
        "3. Combine them into the total loss and use **Adam to update the pixels of `X`**.\n",
        "4. Clamp `X` to `[0,1]` to keep valid image values.\n",
        "5. Every 200 steps, apply a **very light blur** to reduce accumulating high-frequency noise.\n",
        "\n",
        "---\n",
        "\n",
        "#### Output\n",
        "- Saves the final image to `/content/out_painterly.png`.\n",
        "\n",
        "---\n",
        "\n",
        "#### Quick tuning\n",
        "- **Sharper / more structure:** slightly increase `w_content`, reduce or remove the periodic blur, or lower `w_tv`.\n",
        "- **Looser / more painterly:** decrease `w_content`, increase `w_style`, or reduce shallow style layer weights further.\n",
        "- **Less noise:** increase `w_tv` a bit or make the periodic blur slightly more frequent/stronger."
      ],
      "metadata": {
        "id": "2wzrO88hsIay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why there’s a Gaussian blur\n",
        "\n",
        "**Purpose (what it’s for)**  \n",
        "Lightly blur the working image `X` to suppress **tiny, high-frequency details** so optimization prefers **larger, paint-like regions** instead of gritty photo texture.\n",
        "\n",
        "**What a kernel is (and why it’s odd-sized)**  \n",
        "A *kernel* is a small sliding window of weights that averages each pixel with its neighbors.  \n",
        "Odd sizes (3,5,7,…) give a single **center** that aligns with the pixel being updated.\n",
        "\n",
        "**Gaussian blur = bell-curve averaging**  \n",
        "- **Offsets `i`** = positions **relative to the pixel being updated**. If the kernel size is `k` (odd), let `r = (k-1)/2`. Then `i ∈ {-r, …, +r}` are the neighbors left/right (for a row) or up/down (for a column) of the **current pixel**.\n",
        "- **Weights (bell curve):** for each offset `i`, compute an unnormalized weight  \n",
        "  `w_tilde[i] = exp( - i^2 / (2 * sigma^2) )`.  \n",
        "  Then **normalize** so they sum to 1 (keeps brightness):  \n",
        "  `Z = sum_{j=-r}^{r} w_tilde[j]` and `w[i] = w_tilde[i] / Z`.\n",
        "- **Applied to each pixel:** to blur one row at position `(m, n)` you replace the center value with a **weighted average of its neighbors**:  \n",
        "  `X_hat[m, n] = sum_{i=-r}^{r} w[i] * X[m, n - i]`  (horizontal pass)  \n",
        "  and similarly for a **vertical pass** over columns:  \n",
        "  `Y[m, n] = sum_{i=-r}^{r} w[i] * X_hat[m - i, n]`.  \n",
        "  (Two 1-D passes = full 2-D Gaussian blur, faster and equivalent.)\n",
        "- **Intuition:** closer neighbors (small |i|) get larger weights → the pixel becomes a smooth average of its neighborhood, removing tiny high-frequency detail while preserving larger shapes.\n",
        "\n",
        "**From 1-D to 2-D (separable, fast, same result)**  \n",
        "2-D Gaussian kernel is the outer product: `K[i,j] = w[i] * w[j]`.  \n",
        "Convolution can be done as two 1-D passes:  \n",
        "• Vertical: `X_hat[m,n] = sum_{i=-r}^{r} w[i] * X[m-i, n]`  \n",
        "• Horizontal: `Y[m,n] = sum_{j=-r}^{r} w[j] * X_hat[m, n-j]`\n",
        "\n",
        "**How the function `gauss_blur` implements this**  \n",
        "- Builds the 1-D Gaussian weights from `k` (kernel size) and `sigma` (spread) and **normalizes** them.  \n",
        "- Applies **two depthwise convs**: vertical (`k×1`) then horizontal (`1×k`). *Depthwise* means each RGB channel is blurred **independently** (no color mixing).  \n",
        "- Uses padding so the output stays the same size.\n",
        "\n",
        "**How to tune (effect on sharpness)**  \n",
        "- **Sharper (less blur):** smaller `sigma` (e.g., 0.6–1.0), smaller `k` (3–5), or call the blur less often.  \n",
        "- **Softer (more blur):** larger `sigma`/`k`, or apply it more frequently.\n",
        "\n",
        "**Bottom line**  \n",
        "The Gaussian blur is a controlled low-pass filter: it dampens fine noise while keeping overall shapes, helping the style loss form coherent, painterly regions."
      ],
      "metadata": {
        "id": "FR_Luuyt69F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Painterly result: keep forms, suppress photo detail & high-frequency texture\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- weights\n",
        "steps = 500\n",
        "w_content = 1.0\n",
        "w_style  = 12.0\n",
        "w_tv     = 5e-5\n",
        "\n",
        "# --- emphasize mid/deep style layers (coarser patterns), de-emphasize shallow (fine noise)\n",
        "style_layer_weights = {\n",
        "    \"relu1_1\": 0.1,\n",
        "    \"relu2_1\": 0.2,\n",
        "    \"relu3_1\": 1.2,\n",
        "    \"relu4_1\": 1.0,\n",
        "    \"relu5_1\": 0.4,\n",
        "}\n",
        "\n",
        "def total_variation(x):\n",
        "    tv_h = (x[:, :, 1:, :] - x[:, :, :-1, :]).abs().mean()\n",
        "    tv_w = (x[:, :, :, 1:] - x[:, :, :, :-1]).abs().mean()\n",
        "    return tv_h + tv_w\n",
        "\n",
        "def gauss_blur(x, k=7, sigma=2.0):\n",
        "    # cheap separable Gaussian using conv; expects (1,3,H,W)\n",
        "    import math\n",
        "    radius = k // 2\n",
        "    xs = torch.arange(-radius, radius+1, device=x.device, dtype=x.dtype)\n",
        "    w = torch.exp(-0.5 * (xs / sigma) ** 2)\n",
        "    w = (w / w.sum()).view(1, 1, -1)                # (1,1,k)\n",
        "    # depthwise conv along H then W\n",
        "    x = F.conv2d(x, w.unsqueeze(3).expand(3,1,-1,1), padding=(radius,0), groups=3)\n",
        "    x = F.conv2d(x, w.unsqueeze(2).expand(3,1,1,-1), padding=(0,radius), groups=3)\n",
        "    return x\n",
        "\n",
        "# --- init: blurred content + tiny noise (keeps composition, kills crisp detail)\n",
        "with torch.no_grad():\n",
        "  X0 = gauss_blur(C, k=5, sigma=1.0)     # milder blur\n",
        "  X0 = (X0 + 0.005 * torch.randn_like(X0)).clamp(0, 1)  # less noise\n",
        "X = X0.clone().requires_grad_(True)\n",
        "\n",
        "opt = torch.optim.Adam([X], lr=0.02)\n",
        "\n",
        "for t in range(1, steps + 1):\n",
        "    opt.zero_grad()\n",
        "\n",
        "    feats = vgg(X)\n",
        "\n",
        "    # content loss (layout)\n",
        "    Lc = sum(F.mse_loss(feats[n], content_targets[n]) for n in CONTENT_LAYERS)\n",
        "\n",
        "    # style loss (multi-scale, weighted)\n",
        "    Ls = 0.0\n",
        "    for n in STYLE_LAYERS:\n",
        "        Ls = Ls + style_layer_weights[n] * F.mse_loss(gram(feats[n]), style_targets[n])\n",
        "\n",
        "    # smoothness\n",
        "    Ltv = total_variation(X)\n",
        "\n",
        "    loss = w_content*Lc + w_style*Ls + w_tv*Ltv\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # occasional light blur suppresses new high-freq noise while preserving forms\n",
        "        if t % 200 == 0:\n",
        "            X.copy_(gauss_blur(X, k=3, sigma=0.8).clamp(0,1))\n",
        "        X.clamp_(0, 1)\n",
        "\n",
        "    if t % 50 == 0 or t == steps:\n",
        "        print(f\"[{t}/{steps}] Lc={Lc.item():.3f}  Ls={Ls.item():.3f}  Ltv={Ltv.item():.6f}  Total={loss.item():.3f}\")\n",
        "\n",
        "save_image(X, \"/content/out_painterly.png\")\n",
        "print(\"Saved: /content/out_painterly.png\")"
      ],
      "metadata": {
        "id": "FnUyen9ksM5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image.open(\"/content/out_painterly.png\"))"
      ],
      "metadata": {
        "id": "Cjc0LFXdvZEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font color=\"#0b3d91\">Smoothen out the image, add brush strokes</font></h3>"
      ],
      "metadata": {
        "id": "8LPITF-1iEUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -q opencv-contrib-python\n",
        "import cv2, numpy as np\n",
        "\n",
        "def oil_brush_balanced(\n",
        "    in_path, out_path,\n",
        "    brush_size=4, dyn_ratio=70, alpha=0.42,\n",
        "    usm_radius=1.1, usm_amount=1.7,\n",
        "    gamma=1.0,                  # 1.0 = no darkening; <1 brightens, >1 darkens\n",
        "    pre_smooth=False,           # False keeps detail; True = bilateral pre-smooth\n",
        "    add_edges=False, edge_weight=0.14, edge_low=90, edge_high=200\n",
        "):\n",
        "    img = cv2.imread(in_path, cv2.IMREAD_COLOR)\n",
        "    if img is None: raise FileNotFoundError(in_path)\n",
        "\n",
        "    base = cv2.bilateralFilter(img, 7, 30, 7) if pre_smooth else img\n",
        "\n",
        "    # Oil-paint strokes\n",
        "    oil = cv2.xphoto.oilPainting(base, brush_size, dyn_ratio)\n",
        "\n",
        "    # Work in Lab (keep original a/b)\n",
        "    lab_src = cv2.cvtColor(img,  cv2.COLOR_BGR2LAB)\n",
        "    lab_oil = cv2.cvtColor(oil,  cv2.COLOR_BGR2LAB)\n",
        "    Ls = lab_src[...,0].astype(np.float32)\n",
        "    Lo = lab_oil[...,0].astype(np.float32)\n",
        "\n",
        "    # Sharpen painted luminance\n",
        "    blur = cv2.GaussianBlur(Lo, (0,0), usm_radius)\n",
        "    Lo_sharp = (1.0 + usm_amount) * Lo - usm_amount * blur\n",
        "\n",
        "    # Blend THEN tone-match to the original (prevents global darkening)\n",
        "    Lmix = cv2.addWeighted(Lo_sharp, float(alpha), Ls, float(1.0 - alpha), 0.0, dtype=cv2.CV_32F)\n",
        "\n",
        "    # --- replace the mean/std match with histogram matching ---\n",
        "    def _hist_match_to_ref(src_f32, ref_f32):\n",
        "        # map src luminance to have the same histogram as ref (both in 0..255)\n",
        "        src_u8 = np.clip(src_f32, 0, 255).astype(np.uint8)\n",
        "        ref_u8 = np.clip(ref_f32, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # CDFs\n",
        "        src_hist = cv2.calcHist([src_u8], [0], None, [256], [0,256]).ravel()\n",
        "        ref_hist = cv2.calcHist([ref_u8], [0], None, [256], [0,256]).ravel()\n",
        "        src_cdf  = np.cumsum(src_hist) / (src_hist.sum() + 1e-6)\n",
        "        ref_cdf  = np.cumsum(ref_hist) / (ref_hist.sum() + 1e-6)\n",
        "\n",
        "        # For each src intensity, find ref intensity with closest CDF value\n",
        "        mapping = np.interp(src_cdf, ref_cdf, np.arange(256))\n",
        "        return mapping[src_u8].astype(np.float32)\n",
        "\n",
        "    # After you compute Lmix (the blended L), do:\n",
        "    Lmix = _hist_match_to_ref(Lmix, Ls)  # <-- replaces the mean/std normalization\n",
        "\n",
        "    # Optional: darker edge reinforcement (subtle)\n",
        "    if add_edges:\n",
        "        e = cv2.Canny(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), edge_low, edge_high).astype(np.float32)\n",
        "        e = cv2.GaussianBlur(e, (0,0), 0.8)\n",
        "        Lmix = np.clip(Lmix - edge_weight * e, 0, 255)\n",
        "\n",
        "    # Final gamma (1.0 = neutral)\n",
        "    Lmix = np.clip(Lmix/255.0, 0, 1)\n",
        "    Lmix = cv2.pow(Lmix, gamma) * 255.0\n",
        "    Lmix_u8 = np.clip(Lmix, 0, 255).astype(np.uint8)\n",
        "\n",
        "    lab_out = lab_src.copy()\n",
        "    lab_out[...,0] = Lmix_u8\n",
        "    out = cv2.cvtColor(lab_out, cv2.COLOR_LAB2BGR)\n",
        "    cv2.imwrite(out_path, out)\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "in_path  = \"/content/out.png\"\n",
        "out_path = \"/content/out_oil_natural.png\"\n",
        "\n",
        "oil_brush_balanced(\n",
        "    in_path, out_path,\n",
        "    brush_size=4, dyn_ratio=70, alpha=0.40,\n",
        "    usm_radius=1.1, usm_amount=1.8,\n",
        "    gamma=1.0,            # neutral; brightness controlled by hist match\n",
        "    pre_smooth=False,     # keep detail\n",
        "    add_edges=False\n",
        ")\n",
        "\n",
        "display(Image.open(out_path))"
      ],
      "metadata": {
        "id": "OqKYDO9Ias8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open(out_path)\n",
        "w, h = im.size\n",
        "print(f\"Pixels: {w} × {h}\")\n",
        "\n",
        "for ppi in (300, 240, 200, 150):\n",
        "    print(f\"Max print at {ppi} PPI: {w/ppi:.2f} in × {h/ppi:.2f} in\")"
      ],
      "metadata": {
        "id": "sfot0NxDi9QK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}