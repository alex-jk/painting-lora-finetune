{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9Yi5/pl4T6iB30x+SK4oH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/painting-lora-finetune/blob/main/neural_style_transfer_photos_to_paintings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set before importing torch; then Runtime -> Restart runtime\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ],
      "metadata": {
        "id": "a6RMrjh2DzZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CONTENT = \"/content/drive/MyDrive/nst/images/content.jpg\"\n",
        "STYLE   = \"/content/drive/MyDrive/nst/images/style.jpg\"\n",
        "\n",
        "# Option to load generated images from Google Drive\n",
        "generated_out_path = \"/content/drive/MyDrive/nst/images/out.png\"\n",
        "generated_out_painterly_path = \"/content/drive/MyDrive/nst/images/out_painterly.png\"\n",
        "\n",
        "get_generated_images_from_drive = False"
      ],
      "metadata": {
        "id": "w2Iju7g9RVSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font color=\"#0b3d91\">Neural Style Transfer Method</font></h3>\n",
        "\n",
        "**Goal**\n",
        "\n",
        "Turn your photo into a painting-like image without moving objects: keep the photo’s layout, borrow the painting’s colors/texture.\n",
        "\n",
        "**How it works (high level)**<br>\n",
        "We create an output image `X` and optimize its pixels so that:<br>\n",
        "• its **deep features** (from a CNN) match the photo → preserves structure/content;<br>\n",
        "• its **feature statistics** (Gram matrices) match the painting → transfers style/brushwork.\n",
        "\n",
        "**VGG-19**<br>\n",
        "**VGG-19** is a classic **convolutional neural network (CNN)** trained on `ImageNet`. It stacks many conv layers that detect edges, textures, parts, and objects.<br>\n",
        "• **Shallow layers** respond to color/texture; **deep layers** to object/layout.<br>\n",
        "• Freeze VGG (no training) and use it only to extract features that act as \"perceptual rulers.\"<br>\n",
        "• To preserve the photo's content, we match its core VGG features. To inject the painting's style, we match its Gram stats, which are a statistical measurement of which textures and patterns (like certain brushstrokes and colors) tend to appear together throughout the image.\n",
        "\n",
        "<h3><font color=\"#0b3d91\">How the neural style transfer model is trained (fast/feed-forward version)</font></h3>\n",
        "\n",
        "**Notation**\n",
        "- `f_phi`: trainable **stylizer** `CNN` (the only network you update)\n",
        "- `VGG`: frozen **loss network** (used only to compute losses)\n",
        "- `C`: content image, `S`: style image\n",
        "- `X = f_phi(C)`: stylized output\n",
        "\n",
        "**Training loop**\n",
        "1. **Freeze VGG.** Use it only to extract features for the losses (no weight updates).\n",
        "2. **Choose a style** image `S` (or sample from a style set).\n",
        "3. **For each content image `C`:**\n",
        "    - **Forward Pass:** We feed the content photo `C` through our trainable stylizer CNN `f_phi` to produce a candidate image `X`. This network is the one we are actively training, and its output is our `stylized guess`, which we will immediately use to measure how well we are matching our style and content goals.\n",
        "    - **Extract features (with frozen VGG):** get feature maps `F_l(C)`, `F_l(X)`, and `F_l(S)` from selected layers `l`.\n",
        "    - **Content loss:** This is the penalty for changing the photo's core content. We compute it by taking the feature representations of our stylized image `F_l(X)` and the original photo `F_l(C)` from a deeper VGG layer and calculating the squared Euclidean distance (or Mean Squared Error) between them. A smaller distance means the main objects in our guess still look like the original.\n",
        "    - **Style loss:** make **Gram matrices** of `X` match those of `S` across chosen layers (captures texture/brushwork).\n",
        "    - **Total Variation loss (optional):** This is a penalty added to make the final image smoother and less noisy. The actual penalty is the `Total Variation` of the image `X`, which is calculated by summing the differences between each pixel and its immediate neighbors, both horizontally and vertically. A high value means the image has a lot of sharp, disconnected pixel noise, and minimizing it creates more gradual transitions.\n",
        "    - **Total loss:** `L = lambda_c * L_content + lambda_s * L_style + lambda_tv * L_tv`.\n",
        "    - **Backprop:** send gradients through VGG into `f_phi` (VGG stays frozen) and **update only `phi`** (e.g., Adam). *(During training, only change the weights of the stylizer network (f_phi). Do not change the weights of the VGG loss network.)*\n",
        "4. **Repeat** over batches until validation loss plateaus. The trained `f_phi` then stylizes any new `C` in a single forward pass.\n",
        "\n",
        "> **Alternative (optimization-based NST):** don’t learn `f_phi`. Initialize `X` (e.g., `X <- C`) and **optimize the pixels of `X`** directly to minimize the same losses with VGG frozen.\n",
        "\n",
        "\n",
        "**Pipeline**<br>\n",
        "Load & normalize images → run through frozen `VGG` → compute `content loss` (deep layer), `style loss` (Gram matrices across several layers), plus `total variation (TV) smoothing` → then, in an optimization loop, backprop gradients to the image pixels with L-BFGS/Adam. This process is repeated for hundreds of steps, progressively modifying the image to find the pixels that result in the `lowest possible total loss score`.<br>\n",
        "\n",
        "**Total Variation (TV) smoothing** - A regularizer that penalizes rapid pixel-to-pixel changes. It prefers images that are locally smooth (piecewise-smooth) without simply blurring everything."
      ],
      "metadata": {
        "id": "-0NeC4NdFWWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font color=\"#0b3d91\">Gram matrices (style loss)</font></h3>\n",
        "\n",
        "**Setup (for one VGG layer `l`)**\n",
        "\n",
        "To transfer an artistic style, we need a way to measure 'style' itself. A simple feature match isn't enough, because we don't want to copy the objects from the style painting, just its texture and feel. Gram matrices are the solution. They are a mathematical tool that describes an image's style by ignoring object placement and instead capturing which visual patterns (like colors and brushstrokes) tend to appear together. By forcing our new image to have the same Gram matrix as the style painting, we force it to adopt the same artistic signature.\n",
        "\n",
        "- **Get Feature Maps from an Image:**\n",
        "First, we need to extract visual features from an image. Let's take our current version of the output image `X` as the example. We feed it through a specific layer `l` of the `VGG network`.\n",
        "The output, `F_l(X)`, is a set of \"feature maps\": This is a 3D block of data with shape `[C, H, W]`, representing what the VGG layer \"sees\" in our generated image.\n",
        "- **Flatten the Feature Maps:**\n",
        "The next step is to prepare this 3D block of data for the style calculation by collapsing its spatial dimensions `the height H and width W`. We reshape the feature maps `F_l(X)` into a new 2D matrix, `F`.\n",
        "  - This `F` matrix now has a shape of `[C, H*W]`. Each of the `C` rows represents a different feature channel, and the columns represent all the pixel locations. This transformation is essential because it organizes the data so we can easily measure the relationships between the different feature channels, which is the key to capturing style.\n",
        "- **Gram matrix (per layer):**\n",
        "  - Start with features `F_l(X)` of shape `[C, H, W]`.\n",
        "  - Flatten spatial dims → `F` has shape `[C, N]`, where `N = H * W`.\n",
        "  - Indices: `i, j ∈ {1..C}` are **channel indices**; `p ∈ {1..N}` is the **pixel (spatial) index** after flattening.\n",
        "  - Define the `[C, C]` matrix `G` by  \n",
        "    `G[i, j] = (1/N) * sum_{p=1..N} F[i, p] * F[j, p]`.\n",
        "  - This is the **average product across all spatial positions** of channels `i` and `j`.  \n",
        "    Averaging over `p` removes location and keeps **co-activation (texture) statistics**.\n",
        "\n",
        "**Intuition**\n",
        "- Entry `G[i, j]` is the dot product between channel `i` and `j` across all pixels → how strongly they **co-activate**.\n",
        "- Averaging over locations discards exact positions, so Gram matrices capture **texture/brushwork statistics** (which features occur together), not layout.\n",
        "- Matching `G_l(X)` to `G_l(S)` makes `X` use the same color/texture patterns as the style image at that layer’s scale.\n",
        "\n",
        "**Why multiple layers?**\n",
        "- Shallow layers → fine textures/colors; deeper layers → broader patterns.\n",
        "- Summing style losses over several layers gives a **multi-scale** style match.\n",
        "\n",
        "**Style loss formula**\n",
        "\n",
        "The core of the style loss is a direct comparison between two Gram matrices at each VGG layer `l`:\n",
        "  - One matrix, `G_l(S)`, is computed from the original style painting `S`. This is our fixed target; it represents the \"correct\" artistic style for that layer.\n",
        "  - The other matrix, `G_l(X)`, is computed from our current generated image `X`. This represents the style of our work-in-progress.\n",
        "\n",
        "The goal is to change our generated image `X` until its Gram matrix `G_l(X)` becomes as close as possible to the target `G_l(S)`.\n",
        "- Per layer: `L_style_l = || G_l(X) - G_l(S) ||_F^2`\n",
        "- Total style loss: weighted sum over chosen layers.\n",
        "- Normalize by `H*W` (or `C*H*W`) to keep values scale-stable."
      ],
      "metadata": {
        "id": "LnNbZSB3Br4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "JkuuncyXyG4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LSFUnVPsgd9"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision, PIL\n",
        "print(\"Torch:\", torch.__version__, \"| Vision:\", torchvision.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== SETUP =====================\n",
        "import math, os, time\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "WSqes22jy00u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Set device and ImageNet normalization (for VGG-19 features)**\n",
        "\n",
        "- **Device selection:** Use GPU (`cuda`) if available; otherwise fall back to CPU. All tensors/ops must be on the **same device**.\n",
        "- **ImageNet normalization:** VGG-19 expects RGB inputs scaled to **[0,1]** and normalized per channel with:\n",
        "  - mean = `[0.485, 0.456, 0.406]`\n",
        "  - std  = `[0.229, 0.224, 0.225]`\n",
        "<br>We apply it as: `(x - mean) / std` (broadcast per channel over H×W).\n",
        "- **Why:** Feeding the exact normalization used in training keeps VGG feature distributions correct; skipping it can cause unstable optimization or odd colors.\n",
        "- **`.to(device)` on mean/std:** Puts these constants on the same device as your images to avoid device-mismatch errors."
      ],
      "metadata": {
        "id": "8OG7pKx2zG8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], device=device)\n",
        "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], device=device)"
      ],
      "metadata": {
        "id": "8Alx0zfjy47k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**NST Code**"
      ],
      "metadata": {
        "id": "VzSsDDVTywXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image I/O helpers\n",
        "\n",
        "**`load_image(path, target_long_side=None)`**\n",
        "- **Force RGB (3 channels):** keeps shape consistent for VGG / PyTorch.\n",
        "- **Optional shrink (keep aspect ratio):** if `target_long_side` is set, scale the image so its **longer side = target_long_side**.  \n",
        "  *Why:* fewer pixels ⇒ faster, uses less memory. (Halving width & height ≈ **4× fewer pixels**.)\n",
        "- **High-quality resize (`Image.LANCZOS`):** when you shrink, many pixels are merged. A good filter **averages smartly** so diagonals don’t look like **stair-steps (“jaggies”)** and fine textures don’t turn into **wavy stripes**. Cleaner inputs ⇒ cleaner VGG features.\n",
        "- **To tensor in `[0,1]`:** `transforms.ToTensor()` gives a float tensor **(3, H, W)** scaled to `[0,1]`.\n",
        "- **Add batch dim:** `unsqueeze(0)` → **(1, 3, H, W)** because models expect a batch.\n",
        "- **Send to device:** `.to(device)` moves it to CPU or GPU.\n",
        "\n",
        "**`save_image(tensor, path)`**\n",
        "- **Stop tracking gradients:** `detach()` — we’re just saving numbers now.\n",
        "- **Keep valid range:** `clamp(0,1)` so colors aren’t out of bounds.\n",
        "- **Back to CPU & drop batch:** `cpu().squeeze(0)` → **(3, H, W)**.\n",
        "- **Write file:** `transforms.ToPILImage()(x).save(Path(path))`."
      ],
      "metadata": {
        "id": "dkidXX0v2GT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(path, target_long_side=None):\n",
        "    # Open image from disk and ensure it has 3 channels (Red, Green, Blue)\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "    if target_long_side is not None:\n",
        "        w, h = img.size\n",
        "        # Calculate the scaling factor so the longest side equals 'target_long_side'\n",
        "        s = target_long_side / max(w, h)\n",
        "        # Resize the image using Lanczos resampling: a windowed-sinc interpolation. Lanczos is a high-quality method for shrinking images.\n",
        "        # Instead of taking just a few nearby pixels (like bilinear or bicubic), it looks at a larger area around each pixel and averages them in a smart way.\n",
        "        # The result: sharper details and fewer blur artifacts when the image is reduced.\n",
        "        img = img.resize((round(w*s), round(h*s)), Image.LANCZOS)\n",
        "\n",
        "    # Transform pipeline:\n",
        "    # 1. transforms.ToTensor(): Converts PIL Image (H, W, C) range [0, 255] to Torch Tensor (C, H, W) range [0.0, 1.0]\n",
        "    # 2. unsqueeze(0): Adds a batch dimension at index 0 -> Shape becomes [1, 3, H, W]\n",
        "    # 3. to(device): Moves the tensor to the global 'device' (e.g., GPU/CUDA)\n",
        "    x = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
        "    return x\n",
        "\n",
        "def save_image(tensor, path):\n",
        "    # Post-process tensor for saving:\n",
        "    # 1. detach(): Removes tensor from the computational graph (stops gradient tracking)\n",
        "    # 2. clamp(0, 1): Clips values to ensure they are valid colors (fixes potential oversaturation)\n",
        "    # 3. cpu(): Moves data back to system RAM (PIL cannot save from GPU)\n",
        "    # 4. squeeze(0): Removes the batch dimension -> Shape becomes [3, H, W]\n",
        "    x = tensor.detach().clamp(0,1).cpu().squeeze(0)\n",
        "\n",
        "    # Convert the Tensor back to a PIL image (automatically scales 0.0-1.0 to 0-255)\n",
        "    # and save it to the specified filesystem path\n",
        "    transforms.ToPILImage()(x).save(Path(path))"
      ],
      "metadata": {
        "id": "jRGp7hskyxMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalization module\n",
        "\n",
        "**What it does**\n",
        "- Takes an image batch `x` with shape **(N, 3, H, W)** in **[0,1]** and applies **per-channel** ImageNet normalization:  \n",
        "  `x_norm = (x − mean) / std`.\n",
        "- Stores `mean` and `std` as tensors shaped **(1, 3, 1, 1)** so they **broadcast** across batch and spatial dims (H×W).\n",
        "\n",
        "**Why we need it**\n",
        "- **VGG-19** was trained on ImageNet-normalized RGB. Feeding inputs in the same scale makes VGG features **comparable and stable** for style/content losses.\n",
        "- Using `register_buffer(...)` puts `mean/std` on the **right device**, includes them in the **state_dict**, and keeps them **non-trainable** (not updated by the optimizer).\n",
        "\n",
        "*Typical constants:* `mean = [0.485, 0.456, 0.406]`, `std = [0.229, 0.224, 0.225]`."
      ],
      "metadata": {
        "id": "xFfa0R0w30-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        # mean and std are 3-element tensors: [R, G, B].\n",
        "        # We reshape them to [1, 3, 1, 1] so PyTorch can automatically apply the same value to every pixel\n",
        "        # in that channel for any image size or batch size.\n",
        "        self.register_buffer(\"mean\", mean.view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\",  std.view(1, 3, 1, 1))\n",
        "\n",
        "        # register_buffer:\n",
        "        # - These values are part of the module (they get saved with it), but they are NOT trainable (no gradients).\n",
        "        # - When you move the model to GPU/CPU with .to(device), these values move with it automatically.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is an image tensor with shape [batch, 3, H, W].\n",
        "        # For each color channel (R, G, B), subtract the mean and divide by the std. This puts pixel values on the same scale as the images used to train VGG.\n",
        "        return (x - self.mean) / self.std"
      ],
      "metadata": {
        "id": "-3EUQpy81xTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG “taps” — which layers we read and why\n",
        "\n",
        "**What is a tap?**  \n",
        "A *tap* is a layer in VGG where we **read** the activation (feature map) during the forward pass. VGG is **frozen** (not trained). We only use the tapped layers in our losses; **all other layers are ignored**.\n",
        "\n",
        "**Are we using all layers?**  \n",
        "**No.** We select a small subset:\n",
        "- **Content taps** → preserve the photo’s layout/structure.\n",
        "- **Style taps** → capture texture/brush/color statistics at multiple scales.\n",
        "\n",
        "**Our choices**\n",
        "```python\n",
        "# human-readable names for vgg19.features indices\n",
        "VGG_LAYER_NAMES = {1:\"relu1_1\", 6:\"relu2_1\", 11:\"relu3_1\", 20:\"relu4_1\", 22:\"relu4_2\", 29:\"relu5_1\"}\n",
        "\n",
        "# use this deep layer to keep object/layout structure\n",
        "CONTENT_LAYERS = [\"relu4_2\"]\n",
        "\n",
        "# use these (shallow→deep) to capture style across scales\n",
        "STYLE_LAYERS   = [\"relu1_1\",\"relu2_1\",\"relu3_1\",\"relu4_1\",\"relu5_1\"]"
      ],
      "metadata": {
        "id": "Z9pPOe0ENHMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG layer taps\n",
        "VGG_LAYER_NAMES = {1:\"relu1_1\", 6:\"relu2_1\", 11:\"relu3_1\", 20:\"relu4_1\", 22:\"relu4_2\", 29:\"relu5_1\"}\n",
        "CONTENT_LAYERS = [\"relu4_2\"]\n",
        "STYLE_LAYERS   = [\"relu1_1\",\"relu2_1\",\"relu3_1\",\"relu4_1\",\"relu5_1\"]"
      ],
      "metadata": {
        "id": "P_jDS4QFM86v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `VGGFeatures` — frozen VGG wrapper that returns only the layers we care about\n",
        "\n",
        "**Goal:** In one forward pass, (1) normalize the input exactly like ImageNet and (2) collect activations at selected “tap” layers for our losses.\n",
        "\n",
        "**How it’s built (`__init__`):**\n",
        "- Load `vgg19.features` (try the new weights API; fall back to `pretrained=True`).\n",
        "- Call `eval()` and set `requires_grad_(False)` → VGG is **frozen** (we never train it).\n",
        "- Create `Normalization(mean, std)` so inputs are ImageNet-normalized before entering VGG.\n",
        "\n",
        "**What `forward(x)` does:**\n",
        "1. Normalize: `x ← (x − mean) / std`.\n",
        "2. Run through VGG **layer by layer**.\n",
        "3. If the current layer index is listed in `VGG_LAYER_NAMES`, **store that activation** under its readable name (e.g., `\"relu4_2\"`).\n",
        "4. Return a dict mapping names → activations (each `(N, C, H, W)`), e.g.:\n",
        "   `{\"relu1_1\": T1, \"relu2_1\": T2, \"relu3_1\": T3, \"relu4_1\": T4, \"relu4_2\": T5, \"relu5_1\": T6}`.\n",
        "\n",
        "**Why this design:**\n",
        "- We do **not** use all VGG layers. We tap a small subset:\n",
        "  - **Content loss:** compare `X` vs `C` at a deep tap (e.g., `\"relu4_2\"`).\n",
        "  - **Style loss:** compare Gram stats at several shallow→deep taps.\n",
        "- Freezing VGG + correct normalization → features are **stable and comparable**; only your stylizer (or pixels) is updated."
      ],
      "metadata": {
        "id": "cVj4TJ4JR4hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGFeatures(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Load the convolutional part of a pretrained VGG19 network. We use built-in ImageNet weights (IMAGENET1K_V1).\n",
        "        feats = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        # Turn off \"inplace\" for all ReLU layers. This makes PyTorch keep the input and output of ReLU separate,\n",
        "        # which is safer when we need gradients for style transfer.\n",
        "        for m in feats.modules():\n",
        "            if isinstance(m, nn.ReLU):\n",
        "                m.inplace = False\n",
        "\n",
        "        # Put VGG in evaluation mode (no dropout, no batch-norm updates) and move it to the chosen device (CPU/GPU).\n",
        "        self.vgg = feats.eval().to(device)\n",
        "\n",
        "        # Freeze all VGG weights: we never train VGG itself.\n",
        "        for p in self.vgg.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "        # Normalization layer so input images match the statistics (mean and std) of the images VGG was trained on.\n",
        "        self.norm = Normalization(IMAGENET_MEAN, IMAGENET_STD)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Dictionary to store the feature maps we care about.\n",
        "        out = {}\n",
        "\n",
        "        # First normalize the input image.\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Pass through each layer of VGG in order.\n",
        "        for i, layer in enumerate(self.vgg):\n",
        "            x = layer(x)\n",
        "\n",
        "            # If this layer index is one we care about, save its output under a readable name like \"conv1_1\", \"conv2_1\", etc.\n",
        "            if i in VGG_LAYER_NAMES:\n",
        "                out[VGG_LAYER_NAMES[i]] = x\n",
        "\n",
        "        # Return a dict: {layer_name: feature_tensor}\n",
        "        return out\n",
        "\n",
        "\n",
        "def gram(feat):  # Gram in fp32 for stability\n",
        "    # Convert to float32 to avoid numeric issues in the matrix multiply.\n",
        "    feat = feat.float()\n",
        "\n",
        "    # feat has shape [batch, channels, height, width]\n",
        "    _, C, H, W = feat.shape\n",
        "\n",
        "    # Flatten the spatial dimensions so each row is one channel:\n",
        "    # shape becomes [C, H*W].\n",
        "    Fm = feat.view(C, H * W)\n",
        "\n",
        "    # Compute the Gram matrix: [C, C].\n",
        "    # Each entry measures how strongly two channels tend to be \"on\" together.\n",
        "    # We divide by the number of pixels so the result doesn't depend on image size.\n",
        "    return (Fm @ Fm.t()) / (H * W)\n"
      ],
      "metadata": {
        "id": "xgIG6IlrRoun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precompute content & style targets — what this code does\n",
        "\n",
        "1. **Load images** (`C`, `S`) at a manageable size → tensors `(1,3,H,W)` on the correct device.\n",
        "2. **Frozen VGG taps**: build `VGGFeatures()` (it normalizes to ImageNet and returns only the tapped layers).\n",
        "3. **Forward once**: run `C` and `S` through VGG → get dictionaries of tapped activations: `c_feats`, `s_feats`.\n",
        "4. **Content target(s)**: take the content image activations at `CONTENT_LAYERS` (e.g., `relu4_2`). These preserve layout/structure.\n",
        "5. **Style targets**: for each layer in `STYLE_LAYERS`, compute a **Gram matrix** from the style activations:\n",
        "   - reshape features to `(C, N)` where `C`=channels and `N=H*W`;\n",
        "   - build a `(C×C)` matrix of **average channel-wise products** → captures texture/color statistics while ignoring exact positions.\n",
        "   - **Why Gram matrices at multiple style layers?**  \n",
        "Style is about which features co-occur, not where. A Gram matrix captures channel co-activation (second-order stats) and ignores spatial layout, so matching `G_l(X)` to `G_l(S)` transfers texture/brushwork without copying positions. Using several layers (shallow→deep) covers multi-scale style; normalizing by `H×W` keeps losses comparable across sizes.\n",
        "\n",
        "6. **Sanity check**: print shapes of targets to confirm they were created.\n",
        "\n",
        "**Use next:** feed `content_targets` and `style_targets` into your loss:\n",
        "- (a) **Optimize pixels** `X` directly, or\n",
        "- (b) **Train a stylizer network** `f_ϕ` so that `X = f_ϕ(C)`."
      ],
      "metadata": {
        "id": "ufwmKt2IB3Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CONTENT = \"/content/drive/MyDrive/nst/images/content.jpg\"\n",
        "STYLE   = \"/content/drive/MyDrive/nst/images/style.jpg\""
      ],
      "metadata": {
        "id": "EKhJ_w9PNFak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def total_variation(x):\n",
        "    \"\"\"\n",
        "    Total variation (TV) loss.\n",
        "\n",
        "    What it does:\n",
        "    - Measures how \"noisy\" or \"jagged\" an image is.\n",
        "    - It adds up the absolute differences between neighboring pixels (up/down and left/right) and averages them.\n",
        "    - A lower value means smoother images.\n",
        "\n",
        "    How it’s used:\n",
        "    - In style transfer, this is usually applied ONLY to the **generated image**\n",
        "      (the image we are optimizing), not to the content or style reference images.\n",
        "    - It is added as an extra term in the loss function to encourage the\n",
        "      generated image to be smoother and less pixel-noisy.\n",
        "\n",
        "    x: Tensor of shape [batch, 3, H, W].\n",
        "    \"\"\"\n",
        "    # Vertical differences: compare each pixel to the pixel below it\n",
        "    tv_h = (x[:, :, 1:, :] - x[:, :, :-1, :]).abs().mean()\n",
        "\n",
        "    # Horizontal differences: compare each pixel to the pixel to the right\n",
        "    tv_w = (x[:, :, :, 1:] - x[:, :, :, :-1]).abs().mean()\n",
        "\n",
        "    # Total variation is the sum of vertical and horizontal averages\n",
        "    return tv_h + tv_w"
      ],
      "metadata": {
        "id": "Z_huSpyDXb8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not get_generated_images_from_drive:\n",
        "    # We will work with a smaller version of both images so optimization is faster. The longest side of each image will be 512 pixels.\n",
        "    target_long_side = 512\n",
        "\n",
        "    # Load the content and style images as tensors on the correct device.\n",
        "    C = load_image(CONTENT, target_long_side)\n",
        "    S = load_image(STYLE,   target_long_side)\n",
        "\n",
        "    # VGGFeatures wraps a pretrained VGG19 and returns feature maps from selected layers for style/content.\n",
        "    vgg = VGGFeatures()\n",
        "\n",
        "    # Extract feature maps for the fixed content and style reference images.\n",
        "    # We don't need gradients for these (they never change), so we wrap\n",
        "    # this in torch.no_grad() to save memory and speed.\n",
        "    with torch.no_grad():\n",
        "        c_feats = vgg(C)\n",
        "        s_feats = vgg(S)\n",
        "\n",
        "    # Store the \"target\" content features for the chosen content layers.\n",
        "    # These are what we want the generated image to match in terms of content.\n",
        "    content_targets = {n: c_feats[n].detach() for n in CONTENT_LAYERS}\n",
        "\n",
        "    # Store the \"target\" style Gram matrices for the chosen style layers.\n",
        "    # These are what we want the generated image to match in terms of style.\n",
        "    style_targets   = {n: gram(s_feats[n]).detach() for n in STYLE_LAYERS}\n",
        "\n",
        "    print(\"Content taps:\")\n",
        "    for k in content_targets:\n",
        "        print(\" \", k, \":\", tuple(content_targets[k].shape))\n",
        "    print(\"Style Gram targets:\")\n",
        "    for k in STYLE_LAYERS:\n",
        "        print(\" \", k, \":\", tuple(style_targets[k].shape))\n",
        "\n",
        "    # Optimization settings:\n",
        "    # - steps: number of gradient descent steps\n",
        "    # - lr: learning rate for Adam\n",
        "    # - w_content, w_style, w_tv: weights for each loss component\n",
        "    steps, lr = 300, 0.02\n",
        "    w_content, w_style, w_tv = 1.0, 10.0, 1e-5\n",
        "\n",
        "    # X is the image we will actually optimize.\n",
        "    # Start from the content image C (cloned so we don't modify C itself).\n",
        "    # channels_last can give small speedups on some GPUs.\n",
        "    X = C.clone().to(memory_format=torch.channels_last).requires_grad_(True)\n",
        "\n",
        "    # Adam optimizer will update the pixels of X directly.\n",
        "    opt = torch.optim.Adam([X], lr=lr)\n",
        "\n",
        "    # GradScaler + autocast: mixed-precision for speed on GPU, with scaling\n",
        "    # to avoid numerical issues. If no GPU is available, this effectively\n",
        "    # does nothing (enabled=False).\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    for t in range(1, steps + 1):\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Run VGG forward in mixed precision (if on GPU) to speed things up.\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            feats = vgg(X)\n",
        "\n",
        "        # Content loss: how different the generated image features are\n",
        "        # from the content target features, on selected layers.\n",
        "        Lc = sum(\n",
        "            F.mse_loss(feats[n].float(), content_targets[n].float())\n",
        "            for n in CONTENT_LAYERS\n",
        "        )\n",
        "\n",
        "        # Style loss: difference between Gram matrices of the generated image\n",
        "        # and the style targets, on selected layers.\n",
        "        Ls = 0.0\n",
        "        for n in STYLE_LAYERS:\n",
        "            Ls += F.mse_loss(gram(feats[n]), style_targets[n].float())\n",
        "\n",
        "        # Total variation loss: encourages smoothness in the generated image.\n",
        "        # This is applied ONLY to X (the image we’re optimizing).\n",
        "        Ltv = total_variation(X.float())\n",
        "\n",
        "        # Combine the three parts into the final loss.\n",
        "        loss = w_content * Lc + w_style * Ls + w_tv * Ltv\n",
        "\n",
        "        # Backpropagation with gradient scaling (for mixed precision).\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "\n",
        "        # Keep pixel values in the valid range [0, 1] so the image\n",
        "        # stays displayable throughout optimization.\n",
        "        with torch.no_grad():\n",
        "            X.clamp_(0, 1)\n",
        "\n",
        "        # Progress logging every 50 steps and at the very end.\n",
        "        if t % 50 == 0 or t == steps:\n",
        "            print(\n",
        "                f\"[{t}/{steps}] \"\n",
        "                f\"Lc={Lc.item():.4f}  \"\n",
        "                f\"Ls={Ls.item():.4f}  \"\n",
        "                f\"Tv={Ltv.item():.6f}  \"\n",
        "                f\"Total={loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    # Save the final stylized image to disk.\n",
        "    save_image(X, \"out.png\")\n",
        "    print(\"Saved small stylized image -> out.png\")"
      ],
      "metadata": {
        "id": "RfoYcbM7BVBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "if not get_generated_images_from_drive:\n",
        "  display(Image.open(\"out.png\"))"
      ],
      "metadata": {
        "id": "6bRlY0CmRuK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Painterly stylization\n",
        "\n",
        "**Goal**  \n",
        "Create a stylized image that looks more like a painting: keep the overall composition while reducing fine photo detail and “sand” noise.\n",
        "\n",
        "---\n",
        "\n",
        "#### Hyperparameters\n",
        "- `steps = 500` — number of optimization updates.\n",
        "- `w_content = 1.0` — weight for **content loss** (keeps global structure).\n",
        "- `w_style = 12.0` — weight for **style loss** (drives brush/texture/color).\n",
        "- `w_tv = 5e-5` — weight for **total variation loss** (adds smoothness; suppresses speckle).\n",
        "\n",
        "*Effect:* increase `w_style` → stronger style; increase `w_content` → more structure; increase `w_tv` → smoother, less noisy.\n",
        "\n",
        "---\n",
        "\n",
        "#### Style layers (which “style” features to match)\n",
        "- We emphasize **mid/deep taps** (`relu3_1`, `relu4_1`) and down-weight **shallow taps** (`relu1_1`, `relu2_1`, `relu5_1`).\n",
        "- *Why:* shallow layers tend to produce very fine, high-frequency textures; mid/deep taps encourage broader, painterly patterns.\n",
        "\n",
        "---\n",
        "\n",
        "#### Losses computed each step\n",
        "- **Content loss (`Lc`)**: MSE between VGG features of the current image `X` and the content image at `CONTENT_LAYERS` → preserves overall arrangement.\n",
        "- **Style loss (`Ls`)**: MSE between **Gram matrices** of `X` and the style image at `STYLE_LAYERS`, scaled by the per-layer weights → transfers multi-scale texture/brush/color statistics.\n",
        "- **Total variation (`Ltv`)**: encourages neighboring pixels to be similar → reduces pixel-level noise.\n",
        "\n",
        "---\n",
        "\n",
        "#### Gaussian blur helper (`gauss_blur`)\n",
        "- **Gaussian Blur:** An effect that smooths an image by applying a weighted average to each pixel based on its neighbors.\n",
        "- **How it Works:** It uses a kernel, a small matrix of weights that slides over the image to define the blur. A Gaussian kernel's weights follow a bell curve, giving the most importance to the central pixel for a natural-looking effect.\n",
        "- **Implementation:** This function builds a 1D kernel and applies it with two efficient, separable convolutions (vertical then horizontal).\n",
        "- **Why used here:** By applying a very mild blur, it smooths out fine, pixel-level noise across the entire image while being too weak to significantly alter the large, coherent shapes.\n",
        "\n",
        "---\n",
        "\n",
        "#### Initialization\n",
        "- Start `X` from a **mildly blurred** version of the content image plus **tiny noise**.\n",
        "- *Why:* retains composition but softens sharp photo edges, making it easier to form paint-like regions rather than copying photographic detail.\n",
        "\n",
        "---\n",
        "\n",
        "#### Optimization loop (what happens each iteration)\n",
        "1. Run `X` through **frozen VGG** to get tapped features.\n",
        "2. Compute `Lc`, `Ls` (with the style layer weights), and `Ltv`.\n",
        "3. Combine them into the total loss and use **Adam to update the pixels of `X`**.\n",
        "4. Clamp `X` to `[0,1]` to keep valid image values.\n",
        "5. Every 200 steps, apply a **very light blur** to reduce accumulating high-frequency noise.\n",
        "\n",
        "---\n",
        "\n",
        "#### Output\n",
        "- Saves the final image to `/content/out_painterly.png`.\n",
        "\n",
        "---\n",
        "\n",
        "#### Quick tuning\n",
        "- **Sharper / more structure:** slightly increase `w_content`, reduce or remove the periodic blur, or lower `w_tv`.\n",
        "- **Looser / more painterly:** decrease `w_content`, increase `w_style`, or reduce shallow style layer weights further.\n",
        "- **Less noise:** increase `w_tv` a bit or make the periodic blur slightly more frequent/stronger."
      ],
      "metadata": {
        "id": "2wzrO88hsIay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why there’s a Gaussian blur\n",
        "\n",
        "**Purpose (what it’s for)**  \n",
        "Lightly blur the working image `X` to suppress **tiny, high-frequency details** so optimization prefers **larger, paint-like regions** instead of gritty photo texture.\n",
        "\n",
        "**What a kernel is (and why it’s odd-sized)**  \n",
        "A *kernel* is a small sliding window of weights that averages each pixel with its neighbors.  \n",
        "Odd sizes (3,5,7,…) give a single **center** that aligns with the pixel being updated.\n",
        "\n",
        "**Gaussian blur = bell-curve averaging**  \n",
        "- **Offsets `i`** = positions **relative to the pixel being updated**. If the kernel size is `k` (odd), let `r = (k-1)/2`. Then `i ∈ {-r, …, +r}` are the neighbors left/right (for a row) or up/down (for a column) of the **current pixel**.\n",
        "- **Weights (bell curve):** for each offset `i`, compute an unnormalized weight  \n",
        "  `w_tilde[i] = exp( - i^2 / (2 * sigma^2) )`.  \n",
        "  Then **normalize** so they sum to 1 (keeps brightness):  \n",
        "  `Z = sum_{j=-r}^{r} w_tilde[j]` and `w[i] = w_tilde[i] / Z`.\n",
        "- **Applied to each pixel:** to blur one row at position `(m, n)` you replace the center value with a **weighted average of its neighbors**:  \n",
        "  `X_hat[m, n] = sum_{i=-r}^{r} w[i] * X[m, n - i]`  (horizontal pass)  \n",
        "  and similarly for a **vertical pass** over columns:  \n",
        "  `Y[m, n] = sum_{i=-r}^{r} w[i] * X_hat[m - i, n]`.  \n",
        "  (Two 1-D passes = full 2-D Gaussian blur, faster and equivalent.)\n",
        "- **Intuition:** closer neighbors (small |i|) get larger weights → the pixel becomes a smooth average of its neighborhood, removing tiny high-frequency detail while preserving larger shapes.\n",
        "\n",
        "**From 1-D to 2-D (separable, fast, same result)**  \n",
        "2-D Gaussian kernel is the outer product: `K[i,j] = w[i] * w[j]`.  \n",
        "Convolution can be done as two 1-D passes:  \n",
        "• Vertical: `X_hat[m,n] = sum_{i=-r}^{r} w[i] * X[m-i, n]`  \n",
        "• Horizontal: `Y[m,n] = sum_{j=-r}^{r} w[j] * X_hat[m, n-j]`\n",
        "\n",
        "**How the function `gauss_blur` implements this**  \n",
        "- Builds the 1-D Gaussian weights from `k` (kernel size) and `sigma` (spread) and **normalizes** them.  \n",
        "- Applies **two depthwise convs**: vertical (`k×1`) then horizontal (`1×k`). *Depthwise* means each RGB channel is blurred **independently** (no color mixing).  \n",
        "- Uses padding so the output stays the same size.\n",
        "\n",
        "**How to tune (effect on sharpness)**  \n",
        "- **Sharper (less blur):** smaller `sigma` (e.g., 0.6–1.0), smaller `k` (3–5), or call the blur less often.  \n",
        "- **Softer (more blur):** larger `sigma`/`k`, or apply it more frequently.\n",
        "\n",
        "**Bottom line**  \n",
        "The Gaussian blur is a controlled low-pass filter: it dampens fine noise while keeping overall shapes, helping the style loss form coherent, painterly regions."
      ],
      "metadata": {
        "id": "FR_Luuyt69F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not get_generated_images_from_drive:\n",
        "  # Painterly result: keep forms, suppress photo detail & high-frequency texture\n",
        "  import torch\n",
        "  import torch.nn.functional as F\n",
        "\n",
        "  # --- weights\n",
        "  steps = 500\n",
        "  w_content = 1.0\n",
        "  w_style  = 12.0\n",
        "  w_tv     = 5e-4\n",
        "\n",
        "  # --- emphasize mid/deep style layers (coarser patterns), de-emphasize shallow (fine noise)\n",
        "  style_layer_weights = {\n",
        "      \"relu1_1\": 0.1,\n",
        "      \"relu2_1\": 0.2,\n",
        "      \"relu3_1\": 1.2,\n",
        "      \"relu4_1\": 1.0,\n",
        "      \"relu5_1\": 0.4,\n",
        "  }\n",
        "\n",
        "  def total_variation(x):\n",
        "      tv_h = (x[:, :, 1:, :] - x[:, :, :-1, :]).abs().mean()\n",
        "      tv_w = (x[:, :, :, 1:] - x[:, :, :, :-1]).abs().mean()\n",
        "      return tv_h + tv_w\n",
        "\n",
        "  def gauss_blur(x, k=7, sigma=2.0):\n",
        "      # cheap separable Gaussian using conv; expects (1,3,H,W)\n",
        "      import math\n",
        "      radius = k // 2\n",
        "      xs = torch.arange(-radius, radius+1, device=x.device, dtype=x.dtype)\n",
        "      w = torch.exp(-0.5 * (xs / sigma) ** 2)\n",
        "      w = (w / w.sum()).view(1, 1, -1)                # (1,1,k)\n",
        "      # depthwise conv along H then W\n",
        "      x = F.conv2d(x, w.unsqueeze(3).expand(3,1,-1,1), padding=(radius,0), groups=3)\n",
        "      x = F.conv2d(x, w.unsqueeze(2).expand(3,1,1,-1), padding=(0,radius), groups=3)\n",
        "      return x\n",
        "\n",
        "  # --- init: blurred content + tiny noise (keeps composition, kills crisp detail)\n",
        "  with torch.no_grad():\n",
        "    X0 = gauss_blur(C, k=5, sigma=1.0)     # milder blur\n",
        "    X0 = (X0 + 0.005 * torch.randn_like(X0)).clamp(0, 1)  # less noise\n",
        "  X = X0.clone().requires_grad_(True)\n",
        "\n",
        "  opt = torch.optim.Adam([X], lr=0.02)\n",
        "\n",
        "  for t in range(1, steps + 1):\n",
        "      opt.zero_grad()\n",
        "\n",
        "      feats = vgg(X)\n",
        "\n",
        "      # content loss (layout)\n",
        "      Lc = sum(F.mse_loss(feats[n], content_targets[n]) for n in CONTENT_LAYERS)\n",
        "\n",
        "      # style loss (multi-scale, weighted)\n",
        "      Ls = 0.0\n",
        "      for n in STYLE_LAYERS:\n",
        "          Ls = Ls + style_layer_weights[n] * F.mse_loss(gram(feats[n]), style_targets[n])\n",
        "\n",
        "      # smoothness\n",
        "      Ltv = total_variation(X)\n",
        "\n",
        "      loss = w_content*Lc + w_style*Ls + w_tv*Ltv\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # occasional light blur suppresses new high-freq noise while preserving forms\n",
        "          if t % 200 == 0:\n",
        "              X.copy_(gauss_blur(X, k=3, sigma=0.8).clamp(0,1))\n",
        "          X.clamp_(0, 1)\n",
        "\n",
        "      if t % 50 == 0 or t == steps:\n",
        "          print(f\"[{t}/{steps}] Lc={Lc.item():.3f}  Ls={Ls.item():.3f}  Ltv={Ltv.item():.6f}  Total={loss.item():.3f}\")\n",
        "\n",
        "  save_image(X, \"out_painterly.png\")\n",
        "  print(\"Saved: out_painterly.png\")"
      ],
      "metadata": {
        "id": "FnUyen9ksM5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image.open(\"out_painterly.png\"))"
      ],
      "metadata": {
        "id": "Cjc0LFXdvZEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font color=\"#0b3d91\">Enlarge output image</font></h3>"
      ],
      "metadata": {
        "id": "m_0GOEzLReMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== UPSCALE + PER-TILE REFINE (memory-safe, rounding-safe) =====================\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def resize_long_side(img, long_side):\n",
        "    _, _, h, w = img.shape\n",
        "    if max(h, w) == long_side: return img\n",
        "    if h >= w:\n",
        "        new_h, new_w = long_side, int(round(long_side * w / h))\n",
        "    else:\n",
        "        new_w, new_h = long_side, int(round(long_side * h / w))\n",
        "    return F.interpolate(img, size=(new_h, new_w), mode='bicubic', align_corners=False)\n",
        "\n",
        "def make_blend_mask(H, W, overlap, device):\n",
        "    m = torch.ones(1,1,H,W, device=device)\n",
        "    for i in range(overlap):\n",
        "        f = (i+1)/(overlap+1)\n",
        "        m[:,:,i,:]    *= f\n",
        "        m[:,:,-1-i,:] *= f\n",
        "        m[:,:,:,i]    *= f\n",
        "        m[:,:,:,-1-i] *= f\n",
        "    return m\n",
        "\n",
        "def gram32(feat):\n",
        "    feat = feat.float()\n",
        "    _, C, H, W = feat.shape\n",
        "    Fm = feat.view(C, H*W)\n",
        "    return (Fm @ Fm.t()) / (H*W)\n",
        "\n",
        "# ---- 1) Pick your final size; 3000–4000 is plenty for most canvases ----\n",
        "TARGET_LONG = 2400  # e.g. ~13×20\" at 200–300 PPI depending on aspect\n",
        "X_hi = resize_long_side(X.detach(), TARGET_LONG).clamp(0,1)\n",
        "\n",
        "# ---- 2) Precompute low-res (512px canvas) content targets ONCE ----\n",
        "with torch.no_grad():\n",
        "    X_low_full = resize_long_side(X_hi, 512)\n",
        "    c_low_all  = vgg(X_low_full)\n",
        "content_targets_low = {n: c_low_all[n].detach() for n in CONTENT_LAYERS}\n",
        "\n",
        "# ---- 3) Per-tile refine (bounded VRAM) ----\n",
        "tile_size   = 640   # if OOM, drop to 576; if you have L4, try 768–896\n",
        "overlap     = 96\n",
        "tile_steps  = 80\n",
        "tile_lr     = 0.01\n",
        "wC, wS, wTV = 1.0, 30.0, 2e-5\n",
        "\n",
        "H, W = X_hi.shape[2:]\n",
        "stride = tile_size - overlap\n",
        "blend_mask = make_blend_mask(tile_size, tile_size, overlap, X_hi.device)\n",
        "\n",
        "final_num = torch.zeros_like(X_hi)\n",
        "final_den = torch.zeros_like(X_hi)\n",
        "\n",
        "print(f\"Refining {W}x{H} with {tile_size}px tiles (overlap {overlap})...\")\n",
        "\n",
        "stride = tile_size - overlap\n",
        "tiles_h = math.ceil(H / stride)\n",
        "tiles_w = math.ceil(W / stride)\n",
        "tiles_total = tiles_h * tiles_w\n",
        "done = 0\n",
        "print(f\"Tiles: {tiles_h} × {tiles_w} = {tiles_total} total\")\n",
        "\n",
        "for i in range(0, H, stride):\n",
        "    for j in range(0, W, stride):\n",
        "        # tile coords on the big image\n",
        "        i1 = min(i, H - tile_size)\n",
        "        j1 = min(j, W - tile_size)\n",
        "        i2, j2 = i1 + tile_size, j1 + tile_size\n",
        "\n",
        "        # init tile from current canvas\n",
        "        x_tile = X_hi[:, :, i1:i2, j1:j2].clone().requires_grad_(True)\n",
        "        opt_t  = torch.optim.Adam([x_tile], lr=tile_lr)\n",
        "\n",
        "        # ---------- SAFE: build content target crops in feature space ----------\n",
        "        # map big tile -> 512-canvas pixel coords (use floor/ceil + clamp)\n",
        "        sh_512 = 512 / H\n",
        "        sw_512 = 512 / W\n",
        "        ti1_512 = int(math.floor(i1 * sh_512))\n",
        "        ti2_512 = int(math.ceil (i2 * sh_512))\n",
        "        tj1_512 = int(math.floor(j1 * sw_512))\n",
        "        tj2_512 = int(math.ceil (j2 * sw_512))\n",
        "        ti1_512 = max(0, min(ti1_512, 512-1))\n",
        "        tj1_512 = max(0, min(tj1_512, 512-1))\n",
        "        ti2_512 = max(ti1_512+1, min(ti2_512, 512))\n",
        "        tj2_512 = max(tj1_512+1, min(tj2_512, 512))\n",
        "\n",
        "        content_targets_tile = {}\n",
        "        for n in CONTENT_LAYERS:\n",
        "            tgt_full = content_targets_low[n]               # e.g., relu4_2 on 512 canvas\n",
        "            Hf, Wf = tgt_full.shape[2], tgt_full.shape[3]\n",
        "            sh = Hf / X_low_full.shape[2]                   # per-layer scale\n",
        "            sw = Wf / X_low_full.shape[3]\n",
        "            ci1 = int(math.floor(ti1_512 * sh))\n",
        "            ci2 = int(math.ceil (ti2_512 * sh))\n",
        "            cj1 = int(math.floor(tj1_512 * sw))\n",
        "            cj2 = int(math.ceil (tj2_512 * sw))\n",
        "            ci1 = max(0, min(ci1, Hf-1))\n",
        "            cj1 = max(0, min(cj1, Wf-1))\n",
        "            ci2 = max(ci1+1, min(ci2, Hf))\n",
        "            cj2 = max(cj1+1, min(cj2, Wf))\n",
        "            content_targets_tile[n] = tgt_full[:, :, ci1:ci2, cj1:cj2]\n",
        "        # ----------------------------------------------------------------------\n",
        "\n",
        "        for t in range(1, tile_steps+1):\n",
        "            opt_t.zero_grad()\n",
        "\n",
        "            # content: downsample tile to the 512-canvas crop size (>=1x1)\n",
        "            crop_h_512 = max(1, ti2_512 - ti1_512)\n",
        "            crop_w_512 = max(1, tj2_512 - tj1_512)\n",
        "            xt_low = F.interpolate(x_tile, size=(crop_h_512, crop_w_512),\n",
        "                                   mode='bilinear', align_corners=False)\n",
        "            feats_c = vgg(xt_low)\n",
        "\n",
        "            Lc = 0.0\n",
        "            for n in CONTENT_LAYERS:\n",
        "                fn  = feats_c[n].float()\n",
        "                tgt = content_targets_tile[n].float()\n",
        "                th, tw = tgt.shape[2], tgt.shape[3]\n",
        "                if fn.shape[2:] != (th, tw):  # rounding guard\n",
        "                    fn = F.interpolate(fn, size=(max(1,th), max(1,tw)),\n",
        "                                       mode='bilinear', align_corners=False)\n",
        "                Lc += F.mse_loss(fn, tgt)\n",
        "\n",
        "            # style: on the tile (uses your existing global style_targets)\n",
        "            feats_s = vgg(x_tile)\n",
        "            Ls = 0.0\n",
        "            for n in STYLE_LAYERS:\n",
        "                Ls += F.mse_loss(gram32(feats_s[n]), style_targets[n].float())\n",
        "\n",
        "            Ltv = total_variation(x_tile.float())\n",
        "            loss = wC*Lc + wS*Ls + wTV*Ltv\n",
        "            loss.backward()\n",
        "            opt_t.step()\n",
        "            with torch.no_grad():\n",
        "                x_tile.clamp_(0,1)\n",
        "\n",
        "        # feather-blend tile back into the canvas\n",
        "        with torch.no_grad():\n",
        "            m = blend_mask[:, :, :x_tile.shape[2], :x_tile.shape[3]]\n",
        "            final_num[:, :, i1:i2, j1:j2] += x_tile * m\n",
        "            final_den[:, :, i1:i2, j1:j2] += m\n",
        "\n",
        "        done += 1\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "            mem = torch.cuda.memory_allocated()/1e9\n",
        "            print(f\"✔ tile {done}/{tiles_total} | GPU {mem:.2f} GB\")\n",
        "        else:\n",
        "            print(f\"✔ tile {done}/{tiles_total}\")\n",
        "\n",
        "        del x_tile, opt_t, feats_c, feats_s, Lc, Ls, Ltv, loss\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# ---- 4) Reconstruct and save ----\n",
        "X_ref = (final_num / (final_den + 1e-8)).clamp(0,1)\n",
        "save_image(X_ref, \"/content/out_large.png\")\n",
        "print(\"Saved LARGE print image -> /content/out_large.png\")"
      ],
      "metadata": {
        "id": "rFqdjrcJUgmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><font color=\"#0b3d91\">Smoothen out the image, add brush strokes</font></h3>"
      ],
      "metadata": {
        "id": "8LPITF-1iEUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -q opencv-contrib-python\n",
        "import cv2, numpy as np\n",
        "\n",
        "def oil_brush_balanced(\n",
        "    in_path, out_path,\n",
        "    brush_size=4, dyn_ratio=70, alpha=0.42,\n",
        "    usm_radius=1.1, usm_amount=1.7,\n",
        "    gamma=1.0,                  # 1.0 = no darkening; <1 brightens, >1 darkens\n",
        "    pre_smooth=False,           # False keeps detail; True = bilateral pre-smooth\n",
        "    add_edges=False, edge_weight=0.14, edge_low=90, edge_high=200\n",
        "):\n",
        "    img = cv2.imread(in_path, cv2.IMREAD_COLOR)\n",
        "    if img is None: raise FileNotFoundError(in_path)\n",
        "\n",
        "    base = cv2.bilateralFilter(img, 7, 30, 7) if pre_smooth else img\n",
        "\n",
        "    # Oil-paint strokes\n",
        "    oil = cv2.xphoto.oilPainting(base, brush_size, dyn_ratio)\n",
        "\n",
        "    # Work in Lab (keep original a/b)\n",
        "    lab_src = cv2.cvtColor(img,  cv2.COLOR_BGR2LAB)\n",
        "    lab_oil = cv2.cvtColor(oil,  cv2.COLOR_BGR2LAB)\n",
        "    Ls = lab_src[...,0].astype(np.float32)\n",
        "    Lo = lab_oil[...,0].astype(np.float32)\n",
        "\n",
        "    # Sharpen painted luminance\n",
        "    blur = cv2.GaussianBlur(Lo, (0,0), usm_radius)\n",
        "    Lo_sharp = (1.0 + usm_amount) * Lo - usm_amount * blur\n",
        "\n",
        "    # Blend THEN tone-match to the original (prevents global darkening)\n",
        "    Lmix = cv2.addWeighted(Lo_sharp, float(alpha), Ls, float(1.0 - alpha), 0.0, dtype=cv2.CV_32F)\n",
        "\n",
        "    # --- replace the mean/std match with histogram matching ---\n",
        "    def _hist_match_to_ref(src_f32, ref_f32):\n",
        "        # map src luminance to have the same histogram as ref (both in 0..255)\n",
        "        src_u8 = np.clip(src_f32, 0, 255).astype(np.uint8)\n",
        "        ref_u8 = np.clip(ref_f32, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # CDFs\n",
        "        src_hist = cv2.calcHist([src_u8], [0], None, [256], [0,256]).ravel()\n",
        "        ref_hist = cv2.calcHist([ref_u8], [0], None, [256], [0,256]).ravel()\n",
        "        src_cdf  = np.cumsum(src_hist) / (src_hist.sum() + 1e-6)\n",
        "        ref_cdf  = np.cumsum(ref_hist) / (ref_hist.sum() + 1e-6)\n",
        "\n",
        "        # For each src intensity, find ref intensity with closest CDF value\n",
        "        mapping = np.interp(src_cdf, ref_cdf, np.arange(256))\n",
        "        return mapping[src_u8].astype(np.float32)\n",
        "\n",
        "    # After you compute Lmix (the blended L), do:\n",
        "    Lmix = _hist_match_to_ref(Lmix, Ls)  # <-- replaces the mean/std normalization\n",
        "\n",
        "    # Optional: darker edge reinforcement (subtle)\n",
        "    if add_edges:\n",
        "        e = cv2.Canny(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), edge_low, edge_high).astype(np.float32)\n",
        "        e = cv2.GaussianBlur(e, (0,0), 0.8)\n",
        "        Lmix = np.clip(Lmix - edge_weight * e, 0, 255)\n",
        "\n",
        "    # Final gamma (1.0 = neutral)\n",
        "    Lmix = np.clip(Lmix/255.0, 0, 1)\n",
        "    Lmix = cv2.pow(Lmix, gamma) * 255.0\n",
        "    Lmix_u8 = np.clip(Lmix, 0, 255).astype(np.uint8)\n",
        "\n",
        "    lab_out = lab_src.copy()\n",
        "    lab_out[...,0] = Lmix_u8\n",
        "    out = cv2.cvtColor(lab_out, cv2.COLOR_LAB2BGR)\n",
        "    cv2.imwrite(out_path, out)\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "in_path  = \"/content/out.png\"\n",
        "out_path = \"/content/out_oil_natural.png\"\n",
        "\n",
        "oil_brush_balanced(\n",
        "    in_path, out_path,\n",
        "    brush_size=4, dyn_ratio=70, alpha=0.40,\n",
        "    usm_radius=1.1, usm_amount=1.8,\n",
        "    gamma=1.0,            # neutral; brightness controlled by hist match\n",
        "    pre_smooth=False,     # keep detail\n",
        "    add_edges=False\n",
        ")\n",
        "\n",
        "display(Image.open(out_path))"
      ],
      "metadata": {
        "id": "OqKYDO9Ias8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open(out_path)\n",
        "w, h = im.size\n",
        "print(f\"Pixels: {w} × {h}\")\n",
        "\n",
        "for ppi in (300, 240, 200, 150):\n",
        "    print(f\"Max print at {ppi} PPI: {w/ppi:.2f} in × {h/ppi:.2f} in\")"
      ],
      "metadata": {
        "id": "sfot0NxDi9QK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}