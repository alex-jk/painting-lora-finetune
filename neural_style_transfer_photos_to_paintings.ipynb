{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTtLUNo2GVzCyF8f0vRB2p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/painting-lora-finetune/blob/main/neural_style_transfer_photos_to_paintings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "JkuuncyXyG4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6LSFUnVPsgd9",
        "outputId": "84aeac8b-bfe3-4432-adbd-248a9f5552f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 | Vision: 0.23.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Colab usually has recent torch/torchvision, but this is safe.\n",
        "!pip -q install --upgrade torch torchvision pillow\n",
        "import torch, torchvision, PIL\n",
        "print(\"Torch:\", torch.__version__, \"| Vision:\", torchvision.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "WSqes22jy00u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Set device and ImageNet normalization (for VGG-19 features)**\n",
        "\n",
        "- **Device selection:** Use GPU (`cuda`) if available; otherwise fall back to CPU. All tensors/ops must be on the **same device**.\n",
        "- **ImageNet normalization:** VGG-19 expects RGB inputs scaled to **[0,1]** and normalized per channel with:\n",
        "  - mean = `[0.485, 0.456, 0.406]`\n",
        "  - std  = `[0.229, 0.224, 0.225]`\n",
        "<br>We apply it as: `(x - mean) / std` (broadcast per channel over H×W).\n",
        "- **Why:** Feeding the exact normalization used in training keeps VGG feature distributions correct; skipping it can cause unstable optimization or odd colors.\n",
        "- **`.to(device)` on mean/std:** Puts these constants on the same device as your images to avoid device-mismatch errors."
      ],
      "metadata": {
        "id": "8OG7pKx2zG8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).to(device)"
      ],
      "metadata": {
        "id": "8Alx0zfjy47k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**NST Code**"
      ],
      "metadata": {
        "id": "VzSsDDVTywXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image I/O helpers\n",
        "\n",
        "**load_image(path, target_long_side=None)**<br>\n",
        "Convert to **RGB** so every image is 3 channels as VGG expects.<br>\n",
        "Resize by the **longer side** to control compute/memory while keeping aspect ratio; **LANCZOS** gives high-quality downscaling (less aliasing → cleaner features).<br>\n",
        "`ToTensor()` makes a float tensor scaled to **[0,1]**, which is required for the ImageNet normalization we apply later.<br>\n",
        "`unsqueeze(0)` adds a **batch dimension** → models expect shape `(N, C, H, W)` even for a single image.<br>\n",
        "\n",
        "**save_image(tensor, path)**<br>\n",
        "`detach()` drops the autograd graph since we’re done optimizing pixels and just want the data.<br>\n",
        "`clamp(0,1)` enforces valid image range after optimization so saved colors are not out of bounds.<br>\n",
        "`cpu()` moves data to CPU because PIL saves from CPU tensors/arrays.<br>\n",
        "`squeeze(0)` removes the batch dimension; PIL expects `(C, H, W)` (or `(H, W, C)` after conversion).<br>"
      ],
      "metadata": {
        "id": "dkidXX0v2GT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(path, target_long_side=None):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if target_long_side is not None:\n",
        "        w, h = img.size\n",
        "        scale = target_long_side / max(w, h)\n",
        "        img = img.resize((round(w*scale), round(h*scale)), Image.LANCZOS)\n",
        "    x = transforms.ToTensor()(img).unsqueeze(0).to(device)  # (1,3,H,W) in [0,1]\n",
        "    return x\n",
        "\n",
        "def save_image(tensor, path):\n",
        "    x = tensor.detach().clamp(0,1).cpu().squeeze(0)\n",
        "    transforms.ToPILImage()(x).save(Path(path))"
      ],
      "metadata": {
        "id": "jRGp7hskyxMP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization module\n",
        "\n",
        "**What it does**<br>\n",
        "Applies per-channel ImageNet normalization to an input tensor: `(x - mean) / std`.<br>\n",
        "Stores `mean` and `std` reshaped to `(1, 3, 1, 1)` so they broadcast over H×W.\n",
        "\n",
        "**Why we need it**<br>\n",
        "VGG-19 was trained on ImageNet-normalized RGB; matching that distribution makes its features meaningful and stable for NST.<br>\n",
        "`register_buffer(...)` keeps `mean/std` on the right device (move with `.to(device)`), included in `state_dict`, and **not** trainable parameters."
      ],
      "metadata": {
        "id": "xFfa0R0w30-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"mean\", mean.view(1,3,1,1))\n",
        "        self.register_buffer(\"std\",  std.view(1,3,1,1))\n",
        "    def forward(self, x): return (x - self.mean) / self.std"
      ],
      "metadata": {
        "id": "-3EUQpy81xTG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}